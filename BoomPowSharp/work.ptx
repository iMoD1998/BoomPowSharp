//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30300941
// Cuda compilation tools, release 11.4, V11.4.120
// Based on NVVM 7.0.1
//

.version 7.4
.target sm_61
.address_size 64

	// .globl	nano_work

.visible .entry nano_work(
	.param .u64 nano_work_param_0,
	.param .u64 nano_work_param_1,
	.param .u64 nano_work_param_2,
	.param .u64 nano_work_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<1101>;
	.reg .b64 	%rd<1175>;


	ld.param.u64 	%rd3, [nano_work_param_0];
	ld.param.u64 	%rd4, [nano_work_param_2];
	ld.param.u64 	%rd5, [nano_work_param_3];
	cvta.to.global.u64 	%rd6, %rd4;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd7, %r4;
	add.s64 	%rd1, %rd7, %rd3;
	add.s64 	%rd8, %rd1, -4965156021692249135;
	ld.global.u64 	%rd9, [%rd6+8];
	add.s64 	%rd10, %rd9, 6227659224458531674;
	xor.b64  	%rd11, %rd8, 5840696475078001401;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd11, 32;
	shr.b64 	%rhs, %rd11, 32;
	add.u64 	%rd12, %lhs, %rhs;
	}
	xor.b64  	%rd13, %rd10, -7276294671716946913;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd13, 32;
	shr.b64 	%rhs, %rd13, 32;
	add.u64 	%rd14, %lhs, %rhs;
	}
	add.s64 	%rd15, %rd12, 7640891576956012808;
	add.s64 	%rd16, %rd14, -4942790177534073029;
	xor.b64  	%rd17, %rd15, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r5,%dummy}, %rd17;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r6}, %rd17;
	}
	shf.r.wrap.b32 	%r7, %r6, %r5, 24;
	shf.r.wrap.b32 	%r8, %r5, %r6, 24;
	mov.b64 	%rd18, {%r8, %r7};
	xor.b64  	%rd19, %rd16, -7276294671716946913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9,%dummy}, %rd19;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10}, %rd19;
	}
	shf.r.wrap.b32 	%r11, %r10, %r9, 24;
	shf.r.wrap.b32 	%r12, %r9, %r10, 24;
	mov.b64 	%rd20, {%r12, %r11};
	add.s64 	%rd21, %rd18, %rd8;
	ld.global.u64 	%rd22, [%rd6];
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.u64 	%rd24, [%rd6+16];
	add.s64 	%rd25, %rd24, %rd10;
	add.s64 	%rd26, %rd25, %rd20;
	xor.b64  	%rd27, %rd23, %rd12;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r13,%dummy}, %rd27;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r14}, %rd27;
	}
	shf.r.wrap.b32 	%r15, %r14, %r13, 16;
	shf.r.wrap.b32 	%r16, %r13, %r14, 16;
	mov.b64 	%rd28, {%r16, %r15};
	xor.b64  	%rd29, %rd26, %rd14;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r17,%dummy}, %rd29;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r18}, %rd29;
	}
	shf.r.wrap.b32 	%r19, %r18, %r17, 16;
	shf.r.wrap.b32 	%r20, %r17, %r18, 16;
	mov.b64 	%rd30, {%r20, %r19};
	add.s64 	%rd31, %rd28, %rd15;
	add.s64 	%rd32, %rd30, %rd16;
	xor.b64  	%rd33, %rd31, %rd18;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r21}, %rd33;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r22,%dummy}, %rd33;
	}
	shf.l.wrap.b32 	%r23, %r22, %r21, 1;
	shf.l.wrap.b32 	%r24, %r21, %r22, 1;
	mov.b64 	%rd34, {%r24, %r23};
	xor.b64  	%rd35, %rd32, %rd20;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r25}, %rd35;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r26,%dummy}, %rd35;
	}
	shf.l.wrap.b32 	%r27, %r26, %r25, 1;
	shf.l.wrap.b32 	%r28, %r25, %r26, 1;
	mov.b64 	%rd36, {%r28, %r27};
	ld.global.u64 	%rd37, [%rd6+24];
	add.s64 	%rd38, %rd37, 6625583534739731862;
	xor.b64  	%rd39, %rd38, -2270897969802886508;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd39, 32;
	shr.b64 	%rhs, %rd39, 32;
	add.u64 	%rd40, %lhs, %rhs;
	}
	add.s64 	%rd41, %rd40, 4354685564936845355;
	xor.b64  	%rd42, %rd41, 2270897969802886507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r29,%dummy}, %rd42;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r30}, %rd42;
	}
	shf.r.wrap.b32 	%r31, %r30, %r29, 24;
	shf.r.wrap.b32 	%r32, %r29, %r30, 24;
	mov.b64 	%rd43, {%r32, %r31};
	add.s64 	%rd44, %rd43, %rd38;
	xor.b64  	%rd45, %rd44, %rd40;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r33,%dummy}, %rd45;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r34}, %rd45;
	}
	shf.r.wrap.b32 	%r35, %r34, %r33, 16;
	shf.r.wrap.b32 	%r36, %r33, %r34, 16;
	mov.b64 	%rd46, {%r36, %r35};
	add.s64 	%rd47, %rd46, %rd41;
	xor.b64  	%rd48, %rd47, %rd43;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r37}, %rd48;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r38,%dummy}, %rd48;
	}
	shf.l.wrap.b32 	%r39, %r38, %r37, 1;
	shf.l.wrap.b32 	%r40, %r37, %r38, 1;
	mov.b64 	%rd49, {%r40, %r39};
	add.s64 	%rd50, %rd36, %rd23;
	add.s64 	%rd51, %rd49, %rd26;
	xor.b64  	%rd52, %rd50, -6746685272426867986;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd52, 32;
	shr.b64 	%rhs, %rd52, 32;
	add.u64 	%rd53, %lhs, %rhs;
	}
	xor.b64  	%rd54, %rd51, %rd28;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd54, 32;
	shr.b64 	%rhs, %rd54, 32;
	add.u64 	%rd55, %lhs, %rhs;
	}
	add.s64 	%rd56, %rd53, %rd47;
	add.s64 	%rd57, %rd55, -6227242660284835543;
	xor.b64  	%rd58, %rd56, %rd36;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r41,%dummy}, %rd58;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r42}, %rd58;
	}
	shf.r.wrap.b32 	%r43, %r42, %r41, 24;
	shf.r.wrap.b32 	%r44, %r41, %r42, 24;
	mov.b64 	%rd59, {%r44, %r43};
	xor.b64  	%rd60, %rd57, %rd49;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r45,%dummy}, %rd60;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r46}, %rd60;
	}
	shf.r.wrap.b32 	%r47, %r46, %r45, 24;
	shf.r.wrap.b32 	%r48, %r45, %r46, 24;
	mov.b64 	%rd61, {%r48, %r47};
	add.s64 	%rd62, %rd59, %rd50;
	add.s64 	%rd63, %rd61, %rd51;
	xor.b64  	%rd64, %rd62, %rd53;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r49,%dummy}, %rd64;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r50}, %rd64;
	}
	shf.r.wrap.b32 	%r51, %r50, %r49, 16;
	shf.r.wrap.b32 	%r52, %r49, %r50, 16;
	mov.b64 	%rd65, {%r52, %r51};
	xor.b64  	%rd66, %rd63, %rd55;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r53,%dummy}, %rd66;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r54}, %rd66;
	}
	shf.r.wrap.b32 	%r55, %r54, %r53, 16;
	shf.r.wrap.b32 	%r56, %r53, %r54, 16;
	mov.b64 	%rd67, {%r56, %r55};
	add.s64 	%rd68, %rd65, %rd56;
	add.s64 	%rd69, %rd67, %rd57;
	xor.b64  	%rd70, %rd68, %rd59;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r57}, %rd70;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r58,%dummy}, %rd70;
	}
	shf.l.wrap.b32 	%r59, %r58, %r57, 1;
	shf.l.wrap.b32 	%r60, %r57, %r58, 1;
	mov.b64 	%rd71, {%r60, %r59};
	xor.b64  	%rd72, %rd69, %rd61;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r61}, %rd72;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd72;
	}
	shf.l.wrap.b32 	%r63, %r62, %r61, 1;
	shf.l.wrap.b32 	%r64, %r61, %r62, 1;
	mov.b64 	%rd73, {%r64, %r63};
	add.s64 	%rd74, %rd44, 8495551619518569222;
	add.s64 	%rd75, %rd34, -7739430804463375084;
	xor.b64  	%rd76, %rd30, %rd74;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd76, 32;
	shr.b64 	%rhs, %rd76, 32;
	add.u64 	%rd77, %lhs, %rhs;
	}
	xor.b64  	%rd78, %rd46, %rd75;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd78, 32;
	shr.b64 	%rhs, %rd78, 32;
	add.u64 	%rd79, %lhs, %rhs;
	}
	add.s64 	%rd80, %rd77, %rd31;
	add.s64 	%rd81, %rd79, %rd32;
	xor.b64  	%rd82, %rd80, 8495551619518569222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r65,%dummy}, %rd82;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r66}, %rd82;
	}
	shf.r.wrap.b32 	%r67, %r66, %r65, 24;
	shf.r.wrap.b32 	%r68, %r65, %r66, 24;
	mov.b64 	%rd83, {%r68, %r67};
	xor.b64  	%rd84, %rd81, %rd34;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r69,%dummy}, %rd84;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r70}, %rd84;
	}
	shf.r.wrap.b32 	%r71, %r70, %r69, 24;
	shf.r.wrap.b32 	%r72, %r69, %r70, 24;
	mov.b64 	%rd85, {%r72, %r71};
	add.s64 	%rd86, %rd83, %rd74;
	add.s64 	%rd87, %rd85, %rd75;
	xor.b64  	%rd88, %rd86, %rd77;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r73,%dummy}, %rd88;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r74}, %rd88;
	}
	shf.r.wrap.b32 	%r75, %r74, %r73, 16;
	shf.r.wrap.b32 	%r76, %r73, %r74, 16;
	mov.b64 	%rd89, {%r76, %r75};
	xor.b64  	%rd90, %rd87, %rd79;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r77,%dummy}, %rd90;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r78}, %rd90;
	}
	shf.r.wrap.b32 	%r79, %r78, %r77, 16;
	shf.r.wrap.b32 	%r80, %r77, %r78, 16;
	mov.b64 	%rd91, {%r80, %r79};
	add.s64 	%rd92, %rd89, %rd80;
	add.s64 	%rd93, %rd91, %rd81;
	xor.b64  	%rd94, %rd92, %rd83;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r81}, %rd94;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r82,%dummy}, %rd94;
	}
	shf.l.wrap.b32 	%r83, %r82, %r81, 1;
	shf.l.wrap.b32 	%r84, %r81, %r82, 1;
	mov.b64 	%rd95, {%r84, %r83};
	xor.b64  	%rd96, %rd93, %rd85;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r85}, %rd96;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r86,%dummy}, %rd96;
	}
	shf.l.wrap.b32 	%r87, %r86, %r85, 1;
	shf.l.wrap.b32 	%r88, %r85, %r86, 1;
	mov.b64 	%rd97, {%r88, %r87};
	add.s64 	%rd98, %rd97, %rd62;
	add.s64 	%rd99, %rd63, %rd37;
	add.s64 	%rd100, %rd99, %rd71;
	xor.b64  	%rd101, %rd67, %rd98;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd101, 32;
	shr.b64 	%rhs, %rd101, 32;
	add.u64 	%rd102, %lhs, %rhs;
	}
	xor.b64  	%rd103, %rd100, %rd89;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd103, 32;
	shr.b64 	%rhs, %rd103, 32;
	add.u64 	%rd104, %lhs, %rhs;
	}
	add.s64 	%rd105, %rd102, %rd92;
	add.s64 	%rd106, %rd104, %rd93;
	xor.b64  	%rd107, %rd105, %rd97;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r89,%dummy}, %rd107;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r90}, %rd107;
	}
	shf.r.wrap.b32 	%r91, %r90, %r89, 24;
	shf.r.wrap.b32 	%r92, %r89, %r90, 24;
	mov.b64 	%rd108, {%r92, %r91};
	xor.b64  	%rd109, %rd106, %rd71;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r93,%dummy}, %rd109;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r94}, %rd109;
	}
	shf.r.wrap.b32 	%r95, %r94, %r93, 24;
	shf.r.wrap.b32 	%r96, %r93, %r94, 24;
	mov.b64 	%rd110, {%r96, %r95};
	add.s64 	%rd111, %rd108, %rd98;
	add.s64 	%rd112, %rd110, %rd100;
	xor.b64  	%rd113, %rd111, %rd102;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r97,%dummy}, %rd113;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r98}, %rd113;
	}
	shf.r.wrap.b32 	%r99, %r98, %r97, 16;
	shf.r.wrap.b32 	%r100, %r97, %r98, 16;
	mov.b64 	%rd114, {%r100, %r99};
	xor.b64  	%rd115, %rd112, %rd104;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r101,%dummy}, %rd115;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r102}, %rd115;
	}
	shf.r.wrap.b32 	%r103, %r102, %r101, 16;
	shf.r.wrap.b32 	%r104, %r101, %r102, 16;
	mov.b64 	%rd116, {%r104, %r103};
	add.s64 	%rd117, %rd114, %rd105;
	add.s64 	%rd118, %rd116, %rd106;
	xor.b64  	%rd119, %rd117, %rd108;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r105}, %rd119;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r106,%dummy}, %rd119;
	}
	shf.l.wrap.b32 	%r107, %r106, %r105, 1;
	shf.l.wrap.b32 	%r108, %r105, %r106, 1;
	mov.b64 	%rd120, {%r108, %r107};
	xor.b64  	%rd121, %rd118, %rd110;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r109}, %rd121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r110,%dummy}, %rd121;
	}
	shf.l.wrap.b32 	%r111, %r110, %r109, 1;
	shf.l.wrap.b32 	%r112, %r109, %r110, 1;
	mov.b64 	%rd122, {%r112, %r111};
	add.s64 	%rd123, %rd73, %rd86;
	add.s64 	%rd124, %rd95, %rd87;
	xor.b64  	%rd125, %rd123, %rd91;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd125, 32;
	shr.b64 	%rhs, %rd125, 32;
	add.u64 	%rd126, %lhs, %rhs;
	}
	xor.b64  	%rd127, %rd65, %rd124;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd127, 32;
	shr.b64 	%rhs, %rd127, 32;
	add.u64 	%rd128, %lhs, %rhs;
	}
	add.s64 	%rd129, %rd126, %rd68;
	add.s64 	%rd130, %rd69, %rd128;
	xor.b64  	%rd131, %rd129, %rd73;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r113,%dummy}, %rd131;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r114}, %rd131;
	}
	shf.r.wrap.b32 	%r115, %r114, %r113, 24;
	shf.r.wrap.b32 	%r116, %r113, %r114, 24;
	mov.b64 	%rd132, {%r116, %r115};
	xor.b64  	%rd133, %rd130, %rd95;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r117,%dummy}, %rd133;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r118}, %rd133;
	}
	shf.r.wrap.b32 	%r119, %r118, %r117, 24;
	shf.r.wrap.b32 	%r120, %r117, %r118, 24;
	mov.b64 	%rd134, {%r120, %r119};
	add.s64 	%rd135, %rd132, %rd123;
	add.s64 	%rd136, %rd134, %rd124;
	xor.b64  	%rd137, %rd135, %rd126;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r121,%dummy}, %rd137;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r122}, %rd137;
	}
	shf.r.wrap.b32 	%r123, %r122, %r121, 16;
	shf.r.wrap.b32 	%r124, %r121, %r122, 16;
	mov.b64 	%rd138, {%r124, %r123};
	xor.b64  	%rd139, %rd136, %rd128;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r125,%dummy}, %rd139;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r126}, %rd139;
	}
	shf.r.wrap.b32 	%r127, %r126, %r125, 16;
	shf.r.wrap.b32 	%r128, %r125, %r126, 16;
	mov.b64 	%rd140, {%r128, %r127};
	add.s64 	%rd141, %rd138, %rd129;
	add.s64 	%rd142, %rd140, %rd130;
	xor.b64  	%rd143, %rd141, %rd132;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r129}, %rd143;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r130,%dummy}, %rd143;
	}
	shf.l.wrap.b32 	%r131, %r130, %r129, 1;
	shf.l.wrap.b32 	%r132, %r129, %r130, 1;
	mov.b64 	%rd144, {%r132, %r131};
	xor.b64  	%rd145, %rd142, %rd134;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r133}, %rd145;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r134,%dummy}, %rd145;
	}
	shf.l.wrap.b32 	%r135, %r134, %r133, 1;
	shf.l.wrap.b32 	%r136, %r133, %r134, 1;
	mov.b64 	%rd146, {%r136, %r135};
	add.s64 	%rd147, %rd111, %rd22;
	add.s64 	%rd148, %rd147, %rd122;
	add.s64 	%rd149, %rd112, %rd1;
	add.s64 	%rd150, %rd149, %rd144;
	xor.b64  	%rd151, %rd148, %rd140;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd151, 32;
	shr.b64 	%rhs, %rd151, 32;
	add.u64 	%rd152, %lhs, %rhs;
	}
	xor.b64  	%rd153, %rd150, %rd114;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd153, 32;
	shr.b64 	%rhs, %rd153, 32;
	add.u64 	%rd154, %lhs, %rhs;
	}
	add.s64 	%rd155, %rd152, %rd141;
	add.s64 	%rd156, %rd154, %rd142;
	xor.b64  	%rd157, %rd155, %rd122;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r137,%dummy}, %rd157;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r138}, %rd157;
	}
	shf.r.wrap.b32 	%r139, %r138, %r137, 24;
	shf.r.wrap.b32 	%r140, %r137, %r138, 24;
	mov.b64 	%rd158, {%r140, %r139};
	xor.b64  	%rd159, %rd156, %rd144;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r141,%dummy}, %rd159;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r142}, %rd159;
	}
	shf.r.wrap.b32 	%r143, %r142, %r141, 24;
	shf.r.wrap.b32 	%r144, %r141, %r142, 24;
	mov.b64 	%rd160, {%r144, %r143};
	add.s64 	%rd161, %rd158, %rd148;
	add.s64 	%rd162, %rd150, %rd9;
	add.s64 	%rd163, %rd162, %rd160;
	xor.b64  	%rd164, %rd161, %rd152;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r145,%dummy}, %rd164;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r146}, %rd164;
	}
	shf.r.wrap.b32 	%r147, %r146, %r145, 16;
	shf.r.wrap.b32 	%r148, %r145, %r146, 16;
	mov.b64 	%rd165, {%r148, %r147};
	xor.b64  	%rd166, %rd163, %rd154;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r149,%dummy}, %rd166;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r150}, %rd166;
	}
	shf.r.wrap.b32 	%r151, %r150, %r149, 16;
	shf.r.wrap.b32 	%r152, %r149, %r150, 16;
	mov.b64 	%rd167, {%r152, %r151};
	add.s64 	%rd168, %rd165, %rd155;
	add.s64 	%rd169, %rd167, %rd156;
	xor.b64  	%rd170, %rd168, %rd158;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r153}, %rd170;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r154,%dummy}, %rd170;
	}
	shf.l.wrap.b32 	%r155, %r154, %r153, 1;
	shf.l.wrap.b32 	%r156, %r153, %r154, 1;
	mov.b64 	%rd171, {%r156, %r155};
	xor.b64  	%rd172, %rd169, %rd160;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r157}, %rd172;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r158,%dummy}, %rd172;
	}
	shf.l.wrap.b32 	%r159, %r158, %r157, 1;
	shf.l.wrap.b32 	%r160, %r157, %r158, 1;
	mov.b64 	%rd173, {%r160, %r159};
	add.s64 	%rd174, %rd135, %rd146;
	add.s64 	%rd175, %rd120, %rd136;
	xor.b64  	%rd176, %rd116, %rd174;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd176, 32;
	shr.b64 	%rhs, %rd176, 32;
	add.u64 	%rd177, %lhs, %rhs;
	}
	xor.b64  	%rd178, %rd138, %rd175;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd178, 32;
	shr.b64 	%rhs, %rd178, 32;
	add.u64 	%rd179, %lhs, %rhs;
	}
	add.s64 	%rd180, %rd177, %rd117;
	add.s64 	%rd181, %rd179, %rd118;
	xor.b64  	%rd182, %rd180, %rd146;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r161,%dummy}, %rd182;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r162}, %rd182;
	}
	shf.r.wrap.b32 	%r163, %r162, %r161, 24;
	shf.r.wrap.b32 	%r164, %r161, %r162, 24;
	mov.b64 	%rd183, {%r164, %r163};
	xor.b64  	%rd184, %rd181, %rd120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r165,%dummy}, %rd184;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r166}, %rd184;
	}
	shf.r.wrap.b32 	%r167, %r166, %r165, 24;
	shf.r.wrap.b32 	%r168, %r165, %r166, 24;
	mov.b64 	%rd185, {%r168, %r167};
	add.s64 	%rd186, %rd183, %rd174;
	add.s64 	%rd187, %rd175, %rd24;
	add.s64 	%rd188, %rd187, %rd185;
	xor.b64  	%rd189, %rd186, %rd177;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r169,%dummy}, %rd189;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r170}, %rd189;
	}
	shf.r.wrap.b32 	%r171, %r170, %r169, 16;
	shf.r.wrap.b32 	%r172, %r169, %r170, 16;
	mov.b64 	%rd190, {%r172, %r171};
	xor.b64  	%rd191, %rd188, %rd179;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r173,%dummy}, %rd191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r174}, %rd191;
	}
	shf.r.wrap.b32 	%r175, %r174, %r173, 16;
	shf.r.wrap.b32 	%r176, %r173, %r174, 16;
	mov.b64 	%rd192, {%r176, %r175};
	add.s64 	%rd193, %rd190, %rd180;
	add.s64 	%rd194, %rd192, %rd181;
	xor.b64  	%rd195, %rd193, %rd183;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r177}, %rd195;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r178,%dummy}, %rd195;
	}
	shf.l.wrap.b32 	%r179, %r178, %r177, 1;
	shf.l.wrap.b32 	%r180, %r177, %r178, 1;
	mov.b64 	%rd196, {%r180, %r179};
	xor.b64  	%rd197, %rd194, %rd185;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r181}, %rd197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r182,%dummy}, %rd197;
	}
	shf.l.wrap.b32 	%r183, %r182, %r181, 1;
	shf.l.wrap.b32 	%r184, %r181, %r182, 1;
	mov.b64 	%rd198, {%r184, %r183};
	add.s64 	%rd199, %rd198, %rd161;
	add.s64 	%rd200, %rd171, %rd163;
	xor.b64  	%rd201, %rd167, %rd199;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd201, 32;
	shr.b64 	%rhs, %rd201, 32;
	add.u64 	%rd202, %lhs, %rhs;
	}
	xor.b64  	%rd203, %rd200, %rd190;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd203, 32;
	shr.b64 	%rhs, %rd203, 32;
	add.u64 	%rd204, %lhs, %rhs;
	}
	add.s64 	%rd205, %rd202, %rd193;
	add.s64 	%rd206, %rd204, %rd194;
	xor.b64  	%rd207, %rd205, %rd198;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r185,%dummy}, %rd207;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r186}, %rd207;
	}
	shf.r.wrap.b32 	%r187, %r186, %r185, 24;
	shf.r.wrap.b32 	%r188, %r185, %r186, 24;
	mov.b64 	%rd208, {%r188, %r187};
	xor.b64  	%rd209, %rd206, %rd171;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r189,%dummy}, %rd209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r190}, %rd209;
	}
	shf.r.wrap.b32 	%r191, %r190, %r189, 24;
	shf.r.wrap.b32 	%r192, %r189, %r190, 24;
	mov.b64 	%rd210, {%r192, %r191};
	add.s64 	%rd211, %rd208, %rd199;
	add.s64 	%rd212, %rd200, %rd1;
	add.s64 	%rd213, %rd212, %rd210;
	xor.b64  	%rd214, %rd211, %rd202;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r193,%dummy}, %rd214;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r194}, %rd214;
	}
	shf.r.wrap.b32 	%r195, %r194, %r193, 16;
	shf.r.wrap.b32 	%r196, %r193, %r194, 16;
	mov.b64 	%rd215, {%r196, %r195};
	xor.b64  	%rd216, %rd213, %rd204;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r197,%dummy}, %rd216;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r198}, %rd216;
	}
	shf.r.wrap.b32 	%r199, %r198, %r197, 16;
	shf.r.wrap.b32 	%r200, %r197, %r198, 16;
	mov.b64 	%rd217, {%r200, %r199};
	add.s64 	%rd218, %rd215, %rd205;
	add.s64 	%rd219, %rd217, %rd206;
	xor.b64  	%rd220, %rd218, %rd208;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r201}, %rd220;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r202,%dummy}, %rd220;
	}
	shf.l.wrap.b32 	%r203, %r202, %r201, 1;
	shf.l.wrap.b32 	%r204, %r201, %r202, 1;
	mov.b64 	%rd221, {%r204, %r203};
	xor.b64  	%rd222, %rd219, %rd210;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r205}, %rd222;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r206,%dummy}, %rd222;
	}
	shf.l.wrap.b32 	%r207, %r206, %r205, 1;
	shf.l.wrap.b32 	%r208, %r205, %r206, 1;
	mov.b64 	%rd223, {%r208, %r207};
	add.s64 	%rd224, %rd173, %rd186;
	add.s64 	%rd225, %rd196, %rd188;
	xor.b64  	%rd226, %rd224, %rd192;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd226, 32;
	shr.b64 	%rhs, %rd226, 32;
	add.u64 	%rd227, %lhs, %rhs;
	}
	xor.b64  	%rd228, %rd165, %rd225;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd228, 32;
	shr.b64 	%rhs, %rd228, 32;
	add.u64 	%rd229, %lhs, %rhs;
	}
	add.s64 	%rd230, %rd227, %rd168;
	add.s64 	%rd231, %rd169, %rd229;
	xor.b64  	%rd232, %rd230, %rd173;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r209,%dummy}, %rd232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r210}, %rd232;
	}
	shf.r.wrap.b32 	%r211, %r210, %r209, 24;
	shf.r.wrap.b32 	%r212, %r209, %r210, 24;
	mov.b64 	%rd233, {%r212, %r211};
	xor.b64  	%rd234, %rd231, %rd196;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r213,%dummy}, %rd234;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r214}, %rd234;
	}
	shf.r.wrap.b32 	%r215, %r214, %r213, 24;
	shf.r.wrap.b32 	%r216, %r213, %r214, 24;
	mov.b64 	%rd235, {%r216, %r215};
	add.s64 	%rd236, %rd224, %rd9;
	add.s64 	%rd237, %rd236, %rd233;
	add.s64 	%rd238, %rd235, %rd225;
	xor.b64  	%rd239, %rd237, %rd227;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r217,%dummy}, %rd239;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r218}, %rd239;
	}
	shf.r.wrap.b32 	%r219, %r218, %r217, 16;
	shf.r.wrap.b32 	%r220, %r217, %r218, 16;
	mov.b64 	%rd240, {%r220, %r219};
	xor.b64  	%rd241, %rd238, %rd229;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r221,%dummy}, %rd241;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r222}, %rd241;
	}
	shf.r.wrap.b32 	%r223, %r222, %r221, 16;
	shf.r.wrap.b32 	%r224, %r221, %r222, 16;
	mov.b64 	%rd242, {%r224, %r223};
	add.s64 	%rd243, %rd240, %rd230;
	add.s64 	%rd244, %rd242, %rd231;
	xor.b64  	%rd245, %rd243, %rd233;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r225}, %rd245;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r226,%dummy}, %rd245;
	}
	shf.l.wrap.b32 	%r227, %r226, %r225, 1;
	shf.l.wrap.b32 	%r228, %r225, %r226, 1;
	mov.b64 	%rd246, {%r228, %r227};
	xor.b64  	%rd247, %rd244, %rd235;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r229}, %rd247;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r230,%dummy}, %rd247;
	}
	shf.l.wrap.b32 	%r231, %r230, %r229, 1;
	shf.l.wrap.b32 	%r232, %r229, %r230, 1;
	mov.b64 	%rd248, {%r232, %r231};
	add.s64 	%rd249, %rd223, %rd211;
	add.s64 	%rd250, %rd213, %rd24;
	add.s64 	%rd251, %rd250, %rd246;
	xor.b64  	%rd252, %rd249, %rd242;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd252, 32;
	shr.b64 	%rhs, %rd252, 32;
	add.u64 	%rd253, %lhs, %rhs;
	}
	xor.b64  	%rd254, %rd251, %rd215;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd254, 32;
	shr.b64 	%rhs, %rd254, 32;
	add.u64 	%rd255, %lhs, %rhs;
	}
	add.s64 	%rd256, %rd253, %rd243;
	add.s64 	%rd257, %rd255, %rd244;
	xor.b64  	%rd258, %rd256, %rd223;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r233,%dummy}, %rd258;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r234}, %rd258;
	}
	shf.r.wrap.b32 	%r235, %r234, %r233, 24;
	shf.r.wrap.b32 	%r236, %r233, %r234, 24;
	mov.b64 	%rd259, {%r236, %r235};
	xor.b64  	%rd260, %rd257, %rd246;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r237,%dummy}, %rd260;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r238}, %rd260;
	}
	shf.r.wrap.b32 	%r239, %r238, %r237, 24;
	shf.r.wrap.b32 	%r240, %r237, %r238, 24;
	mov.b64 	%rd261, {%r240, %r239};
	add.s64 	%rd262, %rd259, %rd249;
	add.s64 	%rd263, %rd261, %rd251;
	xor.b64  	%rd264, %rd262, %rd253;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r241,%dummy}, %rd264;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r242}, %rd264;
	}
	shf.r.wrap.b32 	%r243, %r242, %r241, 16;
	shf.r.wrap.b32 	%r244, %r241, %r242, 16;
	mov.b64 	%rd265, {%r244, %r243};
	xor.b64  	%rd266, %rd263, %rd255;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r245,%dummy}, %rd266;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r246}, %rd266;
	}
	shf.r.wrap.b32 	%r247, %r246, %r245, 16;
	shf.r.wrap.b32 	%r248, %r245, %r246, 16;
	mov.b64 	%rd267, {%r248, %r247};
	add.s64 	%rd268, %rd265, %rd256;
	add.s64 	%rd269, %rd267, %rd257;
	xor.b64  	%rd270, %rd268, %rd259;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r249}, %rd270;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r250,%dummy}, %rd270;
	}
	shf.l.wrap.b32 	%r251, %r250, %r249, 1;
	shf.l.wrap.b32 	%r252, %r249, %r250, 1;
	mov.b64 	%rd271, {%r252, %r251};
	xor.b64  	%rd272, %rd269, %rd261;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r253}, %rd272;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r254,%dummy}, %rd272;
	}
	shf.l.wrap.b32 	%r255, %r254, %r253, 1;
	shf.l.wrap.b32 	%r256, %r253, %r254, 1;
	mov.b64 	%rd273, {%r256, %r255};
	add.s64 	%rd274, %rd237, %rd248;
	add.s64 	%rd275, %rd221, %rd238;
	xor.b64  	%rd276, %rd217, %rd274;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd276, 32;
	shr.b64 	%rhs, %rd276, 32;
	add.u64 	%rd277, %lhs, %rhs;
	}
	xor.b64  	%rd278, %rd240, %rd275;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd278, 32;
	shr.b64 	%rhs, %rd278, 32;
	add.u64 	%rd279, %lhs, %rhs;
	}
	add.s64 	%rd280, %rd277, %rd218;
	add.s64 	%rd281, %rd279, %rd219;
	xor.b64  	%rd282, %rd280, %rd248;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r257,%dummy}, %rd282;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r258}, %rd282;
	}
	shf.r.wrap.b32 	%r259, %r258, %r257, 24;
	shf.r.wrap.b32 	%r260, %r257, %r258, 24;
	mov.b64 	%rd283, {%r260, %r259};
	xor.b64  	%rd284, %rd281, %rd221;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r261,%dummy}, %rd284;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r262}, %rd284;
	}
	shf.r.wrap.b32 	%r263, %r262, %r261, 24;
	shf.r.wrap.b32 	%r264, %r261, %r262, 24;
	mov.b64 	%rd285, {%r264, %r263};
	add.s64 	%rd286, %rd274, %rd22;
	add.s64 	%rd287, %rd286, %rd283;
	add.s64 	%rd288, %rd275, %rd37;
	add.s64 	%rd289, %rd288, %rd285;
	xor.b64  	%rd290, %rd287, %rd277;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r265,%dummy}, %rd290;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r266}, %rd290;
	}
	shf.r.wrap.b32 	%r267, %r266, %r265, 16;
	shf.r.wrap.b32 	%r268, %r265, %r266, 16;
	mov.b64 	%rd291, {%r268, %r267};
	xor.b64  	%rd292, %rd289, %rd279;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r269,%dummy}, %rd292;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r270}, %rd292;
	}
	shf.r.wrap.b32 	%r271, %r270, %r269, 16;
	shf.r.wrap.b32 	%r272, %r269, %r270, 16;
	mov.b64 	%rd293, {%r272, %r271};
	add.s64 	%rd294, %rd291, %rd280;
	add.s64 	%rd295, %rd293, %rd281;
	xor.b64  	%rd296, %rd294, %rd283;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r273}, %rd296;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r274,%dummy}, %rd296;
	}
	shf.l.wrap.b32 	%r275, %r274, %r273, 1;
	shf.l.wrap.b32 	%r276, %r273, %r274, 1;
	mov.b64 	%rd297, {%r276, %r275};
	xor.b64  	%rd298, %rd295, %rd285;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r277}, %rd298;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r278,%dummy}, %rd298;
	}
	shf.l.wrap.b32 	%r279, %r278, %r277, 1;
	shf.l.wrap.b32 	%r280, %r277, %r278, 1;
	mov.b64 	%rd299, {%r280, %r279};
	add.s64 	%rd300, %rd299, %rd262;
	add.s64 	%rd301, %rd263, %rd24;
	add.s64 	%rd302, %rd301, %rd271;
	xor.b64  	%rd303, %rd267, %rd300;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd303, 32;
	shr.b64 	%rhs, %rd303, 32;
	add.u64 	%rd304, %lhs, %rhs;
	}
	xor.b64  	%rd305, %rd302, %rd291;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd305, 32;
	shr.b64 	%rhs, %rd305, 32;
	add.u64 	%rd306, %lhs, %rhs;
	}
	add.s64 	%rd307, %rd304, %rd294;
	add.s64 	%rd308, %rd306, %rd295;
	xor.b64  	%rd309, %rd307, %rd299;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r281,%dummy}, %rd309;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r282}, %rd309;
	}
	shf.r.wrap.b32 	%r283, %r282, %r281, 24;
	shf.r.wrap.b32 	%r284, %r281, %r282, 24;
	mov.b64 	%rd310, {%r284, %r283};
	xor.b64  	%rd311, %rd308, %rd271;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r285,%dummy}, %rd311;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r286}, %rd311;
	}
	shf.r.wrap.b32 	%r287, %r286, %r285, 24;
	shf.r.wrap.b32 	%r288, %r285, %r286, 24;
	mov.b64 	%rd312, {%r288, %r287};
	add.s64 	%rd313, %rd310, %rd300;
	add.s64 	%rd314, %rd302, %rd22;
	add.s64 	%rd315, %rd314, %rd312;
	xor.b64  	%rd316, %rd313, %rd304;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r289,%dummy}, %rd316;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r290}, %rd316;
	}
	shf.r.wrap.b32 	%r291, %r290, %r289, 16;
	shf.r.wrap.b32 	%r292, %r289, %r290, 16;
	mov.b64 	%rd317, {%r292, %r291};
	xor.b64  	%rd318, %rd315, %rd306;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r293,%dummy}, %rd318;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r294}, %rd318;
	}
	shf.r.wrap.b32 	%r295, %r294, %r293, 16;
	shf.r.wrap.b32 	%r296, %r293, %r294, 16;
	mov.b64 	%rd319, {%r296, %r295};
	add.s64 	%rd320, %rd317, %rd307;
	add.s64 	%rd321, %rd319, %rd308;
	xor.b64  	%rd322, %rd320, %rd310;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r297}, %rd322;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r298,%dummy}, %rd322;
	}
	shf.l.wrap.b32 	%r299, %r298, %r297, 1;
	shf.l.wrap.b32 	%r300, %r297, %r298, 1;
	mov.b64 	%rd323, {%r300, %r299};
	xor.b64  	%rd324, %rd321, %rd312;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r301}, %rd324;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r302,%dummy}, %rd324;
	}
	shf.l.wrap.b32 	%r303, %r302, %r301, 1;
	shf.l.wrap.b32 	%r304, %r301, %r302, 1;
	mov.b64 	%rd325, {%r304, %r303};
	add.s64 	%rd326, %rd273, %rd287;
	add.s64 	%rd327, %rd297, %rd289;
	xor.b64  	%rd328, %rd326, %rd293;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd328, 32;
	shr.b64 	%rhs, %rd328, 32;
	add.u64 	%rd329, %lhs, %rhs;
	}
	xor.b64  	%rd330, %rd327, %rd265;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd330, 32;
	shr.b64 	%rhs, %rd330, 32;
	add.u64 	%rd331, %lhs, %rhs;
	}
	add.s64 	%rd332, %rd329, %rd268;
	add.s64 	%rd333, %rd269, %rd331;
	xor.b64  	%rd334, %rd332, %rd273;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r305,%dummy}, %rd334;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r306}, %rd334;
	}
	shf.r.wrap.b32 	%r307, %r306, %r305, 24;
	shf.r.wrap.b32 	%r308, %r305, %r306, 24;
	mov.b64 	%rd335, {%r308, %r307};
	xor.b64  	%rd336, %rd333, %rd297;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r309,%dummy}, %rd336;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r310}, %rd336;
	}
	shf.r.wrap.b32 	%r311, %r310, %r309, 24;
	shf.r.wrap.b32 	%r312, %r309, %r310, 24;
	mov.b64 	%rd337, {%r312, %r311};
	add.s64 	%rd338, %rd335, %rd326;
	add.s64 	%rd339, %rd337, %rd327;
	xor.b64  	%rd340, %rd338, %rd329;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r313,%dummy}, %rd340;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r314}, %rd340;
	}
	shf.r.wrap.b32 	%r315, %r314, %r313, 16;
	shf.r.wrap.b32 	%r316, %r313, %r314, 16;
	mov.b64 	%rd341, {%r316, %r315};
	xor.b64  	%rd342, %rd339, %rd331;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r317,%dummy}, %rd342;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r318}, %rd342;
	}
	shf.r.wrap.b32 	%r319, %r318, %r317, 16;
	shf.r.wrap.b32 	%r320, %r317, %r318, 16;
	mov.b64 	%rd343, {%r320, %r319};
	add.s64 	%rd344, %rd341, %rd332;
	add.s64 	%rd345, %rd343, %rd333;
	xor.b64  	%rd346, %rd344, %rd335;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r321}, %rd346;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r322,%dummy}, %rd346;
	}
	shf.l.wrap.b32 	%r323, %r322, %r321, 1;
	shf.l.wrap.b32 	%r324, %r321, %r322, 1;
	mov.b64 	%rd347, {%r324, %r323};
	xor.b64  	%rd348, %rd345, %rd337;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r325}, %rd348;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r326,%dummy}, %rd348;
	}
	shf.l.wrap.b32 	%r327, %r326, %r325, 1;
	shf.l.wrap.b32 	%r328, %r325, %r326, 1;
	mov.b64 	%rd349, {%r328, %r327};
	add.s64 	%rd350, %rd313, %rd9;
	add.s64 	%rd351, %rd350, %rd325;
	add.s64 	%rd352, %rd347, %rd315;
	xor.b64  	%rd353, %rd351, %rd343;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd353, 32;
	shr.b64 	%rhs, %rd353, 32;
	add.u64 	%rd354, %lhs, %rhs;
	}
	xor.b64  	%rd355, %rd352, %rd317;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd355, 32;
	shr.b64 	%rhs, %rd355, 32;
	add.u64 	%rd356, %lhs, %rhs;
	}
	add.s64 	%rd357, %rd354, %rd344;
	add.s64 	%rd358, %rd356, %rd345;
	xor.b64  	%rd359, %rd357, %rd325;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r329,%dummy}, %rd359;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r330}, %rd359;
	}
	shf.r.wrap.b32 	%r331, %r330, %r329, 24;
	shf.r.wrap.b32 	%r332, %r329, %r330, 24;
	mov.b64 	%rd360, {%r332, %r331};
	xor.b64  	%rd361, %rd358, %rd347;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r333,%dummy}, %rd361;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r334}, %rd361;
	}
	shf.r.wrap.b32 	%r335, %r334, %r333, 24;
	shf.r.wrap.b32 	%r336, %r333, %r334, 24;
	mov.b64 	%rd362, {%r336, %r335};
	add.s64 	%rd363, %rd360, %rd351;
	add.s64 	%rd364, %rd362, %rd352;
	xor.b64  	%rd365, %rd363, %rd354;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r337,%dummy}, %rd365;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r338}, %rd365;
	}
	shf.r.wrap.b32 	%r339, %r338, %r337, 16;
	shf.r.wrap.b32 	%r340, %r337, %r338, 16;
	mov.b64 	%rd366, {%r340, %r339};
	xor.b64  	%rd367, %rd364, %rd356;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r341,%dummy}, %rd367;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r342}, %rd367;
	}
	shf.r.wrap.b32 	%r343, %r342, %r341, 16;
	shf.r.wrap.b32 	%r344, %r341, %r342, 16;
	mov.b64 	%rd368, {%r344, %r343};
	add.s64 	%rd369, %rd366, %rd357;
	add.s64 	%rd370, %rd368, %rd358;
	xor.b64  	%rd371, %rd369, %rd360;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r345}, %rd371;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r346,%dummy}, %rd371;
	}
	shf.l.wrap.b32 	%r347, %r346, %r345, 1;
	shf.l.wrap.b32 	%r348, %r345, %r346, 1;
	mov.b64 	%rd372, {%r348, %r347};
	xor.b64  	%rd373, %rd370, %rd362;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r349}, %rd373;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r350,%dummy}, %rd373;
	}
	shf.l.wrap.b32 	%r351, %r350, %r349, 1;
	shf.l.wrap.b32 	%r352, %r349, %r350, 1;
	mov.b64 	%rd374, {%r352, %r351};
	add.s64 	%rd375, %rd349, %rd37;
	add.s64 	%rd376, %rd375, %rd338;
	add.s64 	%rd377, %rd323, %rd339;
	xor.b64  	%rd378, %rd319, %rd376;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd378, 32;
	shr.b64 	%rhs, %rd378, 32;
	add.u64 	%rd379, %lhs, %rhs;
	}
	xor.b64  	%rd380, %rd341, %rd377;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd380, 32;
	shr.b64 	%rhs, %rd380, 32;
	add.u64 	%rd381, %lhs, %rhs;
	}
	add.s64 	%rd382, %rd379, %rd320;
	add.s64 	%rd383, %rd381, %rd321;
	xor.b64  	%rd384, %rd382, %rd349;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r353,%dummy}, %rd384;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r354}, %rd384;
	}
	shf.r.wrap.b32 	%r355, %r354, %r353, 24;
	shf.r.wrap.b32 	%r356, %r353, %r354, 24;
	mov.b64 	%rd385, {%r356, %r355};
	xor.b64  	%rd386, %rd383, %rd323;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r357,%dummy}, %rd386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r358}, %rd386;
	}
	shf.r.wrap.b32 	%r359, %r358, %r357, 24;
	shf.r.wrap.b32 	%r360, %r357, %r358, 24;
	mov.b64 	%rd387, {%r360, %r359};
	add.s64 	%rd388, %rd376, %rd1;
	add.s64 	%rd389, %rd388, %rd385;
	add.s64 	%rd390, %rd387, %rd377;
	xor.b64  	%rd391, %rd389, %rd379;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r361,%dummy}, %rd391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r362}, %rd391;
	}
	shf.r.wrap.b32 	%r363, %r362, %r361, 16;
	shf.r.wrap.b32 	%r364, %r361, %r362, 16;
	mov.b64 	%rd392, {%r364, %r363};
	xor.b64  	%rd393, %rd390, %rd381;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r365,%dummy}, %rd393;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r366}, %rd393;
	}
	shf.r.wrap.b32 	%r367, %r366, %r365, 16;
	shf.r.wrap.b32 	%r368, %r365, %r366, 16;
	mov.b64 	%rd394, {%r368, %r367};
	add.s64 	%rd395, %rd392, %rd382;
	add.s64 	%rd396, %rd394, %rd383;
	xor.b64  	%rd397, %rd395, %rd385;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r369}, %rd397;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r370,%dummy}, %rd397;
	}
	shf.l.wrap.b32 	%r371, %r370, %r369, 1;
	shf.l.wrap.b32 	%r372, %r369, %r370, 1;
	mov.b64 	%rd398, {%r372, %r371};
	xor.b64  	%rd399, %rd396, %rd387;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r373}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r374,%dummy}, %rd399;
	}
	shf.l.wrap.b32 	%r375, %r374, %r373, 1;
	shf.l.wrap.b32 	%r376, %r373, %r374, 1;
	mov.b64 	%rd400, {%r376, %r375};
	add.s64 	%rd401, %rd400, %rd363;
	add.s64 	%rd402, %rd372, %rd364;
	xor.b64  	%rd403, %rd368, %rd401;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd403, 32;
	shr.b64 	%rhs, %rd403, 32;
	add.u64 	%rd404, %lhs, %rhs;
	}
	xor.b64  	%rd405, %rd402, %rd392;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd405, 32;
	shr.b64 	%rhs, %rd405, 32;
	add.u64 	%rd406, %lhs, %rhs;
	}
	add.s64 	%rd407, %rd404, %rd395;
	add.s64 	%rd408, %rd406, %rd396;
	xor.b64  	%rd409, %rd407, %rd400;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r377,%dummy}, %rd409;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r378}, %rd409;
	}
	shf.r.wrap.b32 	%r379, %r378, %r377, 24;
	shf.r.wrap.b32 	%r380, %r377, %r378, 24;
	mov.b64 	%rd410, {%r380, %r379};
	xor.b64  	%rd411, %rd408, %rd372;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r381,%dummy}, %rd411;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r382}, %rd411;
	}
	shf.r.wrap.b32 	%r383, %r382, %r381, 24;
	shf.r.wrap.b32 	%r384, %r381, %r382, 24;
	mov.b64 	%rd412, {%r384, %r383};
	add.s64 	%rd413, %rd401, %rd1;
	add.s64 	%rd414, %rd413, %rd410;
	add.s64 	%rd415, %rd412, %rd402;
	xor.b64  	%rd416, %rd414, %rd404;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r385,%dummy}, %rd416;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r386}, %rd416;
	}
	shf.r.wrap.b32 	%r387, %r386, %r385, 16;
	shf.r.wrap.b32 	%r388, %r385, %r386, 16;
	mov.b64 	%rd417, {%r388, %r387};
	xor.b64  	%rd418, %rd415, %rd406;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r389,%dummy}, %rd418;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r390}, %rd418;
	}
	shf.r.wrap.b32 	%r391, %r390, %r389, 16;
	shf.r.wrap.b32 	%r392, %r389, %r390, 16;
	mov.b64 	%rd419, {%r392, %r391};
	add.s64 	%rd420, %rd417, %rd407;
	add.s64 	%rd421, %rd419, %rd408;
	xor.b64  	%rd422, %rd420, %rd410;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r393}, %rd422;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r394,%dummy}, %rd422;
	}
	shf.l.wrap.b32 	%r395, %r394, %r393, 1;
	shf.l.wrap.b32 	%r396, %r393, %r394, 1;
	mov.b64 	%rd423, {%r396, %r395};
	xor.b64  	%rd424, %rd421, %rd412;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r397}, %rd424;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r398,%dummy}, %rd424;
	}
	shf.l.wrap.b32 	%r399, %r398, %r397, 1;
	shf.l.wrap.b32 	%r400, %r397, %r398, 1;
	mov.b64 	%rd425, {%r400, %r399};
	add.s64 	%rd426, %rd389, %rd9;
	add.s64 	%rd427, %rd426, %rd374;
	add.s64 	%rd428, %rd398, %rd390;
	xor.b64  	%rd429, %rd427, %rd394;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd429, 32;
	shr.b64 	%rhs, %rd429, 32;
	add.u64 	%rd430, %lhs, %rhs;
	}
	xor.b64  	%rd431, %rd366, %rd428;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd431, 32;
	shr.b64 	%rhs, %rd431, 32;
	add.u64 	%rd432, %lhs, %rhs;
	}
	add.s64 	%rd433, %rd430, %rd369;
	add.s64 	%rd434, %rd432, %rd370;
	xor.b64  	%rd435, %rd433, %rd374;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r401,%dummy}, %rd435;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r402}, %rd435;
	}
	shf.r.wrap.b32 	%r403, %r402, %r401, 24;
	shf.r.wrap.b32 	%r404, %r401, %r402, 24;
	mov.b64 	%rd436, {%r404, %r403};
	xor.b64  	%rd437, %rd434, %rd398;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r405,%dummy}, %rd437;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r406}, %rd437;
	}
	shf.r.wrap.b32 	%r407, %r406, %r405, 24;
	shf.r.wrap.b32 	%r408, %r405, %r406, 24;
	mov.b64 	%rd438, {%r408, %r407};
	add.s64 	%rd439, %rd427, %rd37;
	add.s64 	%rd440, %rd439, %rd436;
	add.s64 	%rd441, %rd438, %rd428;
	xor.b64  	%rd442, %rd440, %rd430;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r409,%dummy}, %rd442;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r410}, %rd442;
	}
	shf.r.wrap.b32 	%r411, %r410, %r409, 16;
	shf.r.wrap.b32 	%r412, %r409, %r410, 16;
	mov.b64 	%rd443, {%r412, %r411};
	xor.b64  	%rd444, %rd441, %rd432;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r413,%dummy}, %rd444;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r414}, %rd444;
	}
	shf.r.wrap.b32 	%r415, %r414, %r413, 16;
	shf.r.wrap.b32 	%r416, %r413, %r414, 16;
	mov.b64 	%rd445, {%r416, %r415};
	add.s64 	%rd446, %rd443, %rd433;
	add.s64 	%rd447, %rd445, %rd434;
	xor.b64  	%rd448, %rd446, %rd436;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r417}, %rd448;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r418,%dummy}, %rd448;
	}
	shf.l.wrap.b32 	%r419, %r418, %r417, 1;
	shf.l.wrap.b32 	%r420, %r417, %r418, 1;
	mov.b64 	%rd449, {%r420, %r419};
	xor.b64  	%rd450, %rd447, %rd438;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r421}, %rd450;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r422,%dummy}, %rd450;
	}
	shf.l.wrap.b32 	%r423, %r422, %r421, 1;
	shf.l.wrap.b32 	%r424, %r421, %r422, 1;
	mov.b64 	%rd451, {%r424, %r423};
	add.s64 	%rd452, %rd425, %rd414;
	add.s64 	%rd453, %rd449, %rd415;
	xor.b64  	%rd454, %rd452, %rd445;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd454, 32;
	shr.b64 	%rhs, %rd454, 32;
	add.u64 	%rd455, %lhs, %rhs;
	}
	xor.b64  	%rd456, %rd453, %rd417;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd456, 32;
	shr.b64 	%rhs, %rd456, 32;
	add.u64 	%rd457, %lhs, %rhs;
	}
	add.s64 	%rd458, %rd455, %rd446;
	add.s64 	%rd459, %rd457, %rd447;
	xor.b64  	%rd460, %rd458, %rd425;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r425,%dummy}, %rd460;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r426}, %rd460;
	}
	shf.r.wrap.b32 	%r427, %r426, %r425, 24;
	shf.r.wrap.b32 	%r428, %r425, %r426, 24;
	mov.b64 	%rd461, {%r428, %r427};
	xor.b64  	%rd462, %rd459, %rd449;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r429,%dummy}, %rd462;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r430}, %rd462;
	}
	shf.r.wrap.b32 	%r431, %r430, %r429, 24;
	shf.r.wrap.b32 	%r432, %r429, %r430, 24;
	mov.b64 	%rd463, {%r432, %r431};
	add.s64 	%rd464, %rd452, %rd22;
	add.s64 	%rd465, %rd464, %rd461;
	add.s64 	%rd466, %rd463, %rd453;
	xor.b64  	%rd467, %rd465, %rd455;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r433,%dummy}, %rd467;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r434}, %rd467;
	}
	shf.r.wrap.b32 	%r435, %r434, %r433, 16;
	shf.r.wrap.b32 	%r436, %r433, %r434, 16;
	mov.b64 	%rd468, {%r436, %r435};
	xor.b64  	%rd469, %rd466, %rd457;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r437,%dummy}, %rd469;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r438}, %rd469;
	}
	shf.r.wrap.b32 	%r439, %r438, %r437, 16;
	shf.r.wrap.b32 	%r440, %r437, %r438, 16;
	mov.b64 	%rd470, {%r440, %r439};
	add.s64 	%rd471, %rd468, %rd458;
	add.s64 	%rd472, %rd470, %rd459;
	xor.b64  	%rd473, %rd471, %rd461;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r441}, %rd473;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r442,%dummy}, %rd473;
	}
	shf.l.wrap.b32 	%r443, %r442, %r441, 1;
	shf.l.wrap.b32 	%r444, %r441, %r442, 1;
	mov.b64 	%rd474, {%r444, %r443};
	xor.b64  	%rd475, %rd472, %rd463;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r445}, %rd475;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r446,%dummy}, %rd475;
	}
	shf.l.wrap.b32 	%r447, %r446, %r445, 1;
	shf.l.wrap.b32 	%r448, %r445, %r446, 1;
	mov.b64 	%rd476, {%r448, %r447};
	add.s64 	%rd477, %rd440, %rd451;
	add.s64 	%rd478, %rd441, %rd24;
	add.s64 	%rd479, %rd478, %rd423;
	xor.b64  	%rd480, %rd419, %rd477;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd480, 32;
	shr.b64 	%rhs, %rd480, 32;
	add.u64 	%rd481, %lhs, %rhs;
	}
	xor.b64  	%rd482, %rd443, %rd479;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd482, 32;
	shr.b64 	%rhs, %rd482, 32;
	add.u64 	%rd483, %lhs, %rhs;
	}
	add.s64 	%rd484, %rd481, %rd420;
	add.s64 	%rd485, %rd483, %rd421;
	xor.b64  	%rd486, %rd484, %rd451;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r449,%dummy}, %rd486;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r450}, %rd486;
	}
	shf.r.wrap.b32 	%r451, %r450, %r449, 24;
	shf.r.wrap.b32 	%r452, %r449, %r450, 24;
	mov.b64 	%rd487, {%r452, %r451};
	xor.b64  	%rd488, %rd485, %rd423;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r453,%dummy}, %rd488;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r454}, %rd488;
	}
	shf.r.wrap.b32 	%r455, %r454, %r453, 24;
	shf.r.wrap.b32 	%r456, %r453, %r454, 24;
	mov.b64 	%rd489, {%r456, %r455};
	add.s64 	%rd490, %rd487, %rd477;
	add.s64 	%rd491, %rd489, %rd479;
	xor.b64  	%rd492, %rd490, %rd481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r457,%dummy}, %rd492;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r458}, %rd492;
	}
	shf.r.wrap.b32 	%r459, %r458, %r457, 16;
	shf.r.wrap.b32 	%r460, %r457, %r458, 16;
	mov.b64 	%rd493, {%r460, %r459};
	xor.b64  	%rd494, %rd491, %rd483;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r461,%dummy}, %rd494;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r462}, %rd494;
	}
	shf.r.wrap.b32 	%r463, %r462, %r461, 16;
	shf.r.wrap.b32 	%r464, %r461, %r462, 16;
	mov.b64 	%rd495, {%r464, %r463};
	add.s64 	%rd496, %rd493, %rd484;
	add.s64 	%rd497, %rd495, %rd485;
	xor.b64  	%rd498, %rd496, %rd487;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r465}, %rd498;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r466,%dummy}, %rd498;
	}
	shf.l.wrap.b32 	%r467, %r466, %r465, 1;
	shf.l.wrap.b32 	%r468, %r465, %r466, 1;
	mov.b64 	%rd499, {%r468, %r467};
	xor.b64  	%rd500, %rd497, %rd489;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r469}, %rd500;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r470,%dummy}, %rd500;
	}
	shf.l.wrap.b32 	%r471, %r470, %r469, 1;
	shf.l.wrap.b32 	%r472, %r469, %r470, 1;
	mov.b64 	%rd501, {%r472, %r471};
	add.s64 	%rd502, %rd465, %rd9;
	add.s64 	%rd503, %rd502, %rd501;
	add.s64 	%rd504, %rd474, %rd466;
	xor.b64  	%rd505, %rd503, %rd470;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd505, 32;
	shr.b64 	%rhs, %rd505, 32;
	add.u64 	%rd506, %lhs, %rhs;
	}
	xor.b64  	%rd507, %rd504, %rd493;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd507, 32;
	shr.b64 	%rhs, %rd507, 32;
	add.u64 	%rd508, %lhs, %rhs;
	}
	add.s64 	%rd509, %rd506, %rd496;
	add.s64 	%rd510, %rd508, %rd497;
	xor.b64  	%rd511, %rd509, %rd501;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r473,%dummy}, %rd511;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r474}, %rd511;
	}
	shf.r.wrap.b32 	%r475, %r474, %r473, 24;
	shf.r.wrap.b32 	%r476, %r473, %r474, 24;
	mov.b64 	%rd512, {%r476, %r475};
	xor.b64  	%rd513, %rd510, %rd474;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r477,%dummy}, %rd513;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r478}, %rd513;
	}
	shf.r.wrap.b32 	%r479, %r478, %r477, 24;
	shf.r.wrap.b32 	%r480, %r477, %r478, 24;
	mov.b64 	%rd514, {%r480, %r479};
	add.s64 	%rd515, %rd512, %rd503;
	add.s64 	%rd516, %rd514, %rd504;
	xor.b64  	%rd517, %rd515, %rd506;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r481,%dummy}, %rd517;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r482}, %rd517;
	}
	shf.r.wrap.b32 	%r483, %r482, %r481, 16;
	shf.r.wrap.b32 	%r484, %r481, %r482, 16;
	mov.b64 	%rd518, {%r484, %r483};
	xor.b64  	%rd519, %rd516, %rd508;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r485,%dummy}, %rd519;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r486}, %rd519;
	}
	shf.r.wrap.b32 	%r487, %r486, %r485, 16;
	shf.r.wrap.b32 	%r488, %r485, %r486, 16;
	mov.b64 	%rd520, {%r488, %r487};
	add.s64 	%rd521, %rd518, %rd509;
	add.s64 	%rd522, %rd520, %rd510;
	xor.b64  	%rd523, %rd521, %rd512;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r489}, %rd523;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r490,%dummy}, %rd523;
	}
	shf.l.wrap.b32 	%r491, %r490, %r489, 1;
	shf.l.wrap.b32 	%r492, %r489, %r490, 1;
	mov.b64 	%rd524, {%r492, %r491};
	xor.b64  	%rd525, %rd522, %rd514;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r493}, %rd525;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r494,%dummy}, %rd525;
	}
	shf.l.wrap.b32 	%r495, %r494, %r493, 1;
	shf.l.wrap.b32 	%r496, %r493, %r494, 1;
	mov.b64 	%rd526, {%r496, %r495};
	add.s64 	%rd527, %rd490, %rd1;
	add.s64 	%rd528, %rd527, %rd476;
	add.s64 	%rd529, %rd499, %rd491;
	xor.b64  	%rd530, %rd528, %rd495;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd530, 32;
	shr.b64 	%rhs, %rd530, 32;
	add.u64 	%rd531, %lhs, %rhs;
	}
	xor.b64  	%rd532, %rd468, %rd529;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd532, 32;
	shr.b64 	%rhs, %rd532, 32;
	add.u64 	%rd533, %lhs, %rhs;
	}
	add.s64 	%rd534, %rd531, %rd471;
	add.s64 	%rd535, %rd533, %rd472;
	xor.b64  	%rd536, %rd534, %rd476;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r497,%dummy}, %rd536;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r498}, %rd536;
	}
	shf.r.wrap.b32 	%r499, %r498, %r497, 24;
	shf.r.wrap.b32 	%r500, %r497, %r498, 24;
	mov.b64 	%rd537, {%r500, %r499};
	xor.b64  	%rd538, %rd535, %rd499;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r501,%dummy}, %rd538;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r502}, %rd538;
	}
	shf.r.wrap.b32 	%r503, %r502, %r501, 24;
	shf.r.wrap.b32 	%r504, %r501, %r502, 24;
	mov.b64 	%rd539, {%r504, %r503};
	add.s64 	%rd540, %rd537, %rd528;
	add.s64 	%rd541, %rd529, %rd24;
	add.s64 	%rd542, %rd541, %rd539;
	xor.b64  	%rd543, %rd540, %rd531;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r505,%dummy}, %rd543;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r506}, %rd543;
	}
	shf.r.wrap.b32 	%r507, %r506, %r505, 16;
	shf.r.wrap.b32 	%r508, %r505, %r506, 16;
	mov.b64 	%rd544, {%r508, %r507};
	xor.b64  	%rd545, %rd542, %rd533;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r509,%dummy}, %rd545;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r510}, %rd545;
	}
	shf.r.wrap.b32 	%r511, %r510, %r509, 16;
	shf.r.wrap.b32 	%r512, %r509, %r510, 16;
	mov.b64 	%rd546, {%r512, %r511};
	add.s64 	%rd547, %rd544, %rd534;
	add.s64 	%rd548, %rd546, %rd535;
	xor.b64  	%rd549, %rd547, %rd537;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r513}, %rd549;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r514,%dummy}, %rd549;
	}
	shf.l.wrap.b32 	%r515, %r514, %r513, 1;
	shf.l.wrap.b32 	%r516, %r513, %r514, 1;
	mov.b64 	%rd550, {%r516, %r515};
	xor.b64  	%rd551, %rd548, %rd539;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r517}, %rd551;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r518,%dummy}, %rd551;
	}
	shf.l.wrap.b32 	%r519, %r518, %r517, 1;
	shf.l.wrap.b32 	%r520, %r517, %r518, 1;
	mov.b64 	%rd552, {%r520, %r519};
	add.s64 	%rd553, %rd515, %rd37;
	add.s64 	%rd554, %rd553, %rd526;
	add.s64 	%rd555, %rd550, %rd516;
	xor.b64  	%rd556, %rd554, %rd546;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd556, 32;
	shr.b64 	%rhs, %rd556, 32;
	add.u64 	%rd557, %lhs, %rhs;
	}
	xor.b64  	%rd558, %rd555, %rd518;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd558, 32;
	shr.b64 	%rhs, %rd558, 32;
	add.u64 	%rd559, %lhs, %rhs;
	}
	add.s64 	%rd560, %rd557, %rd547;
	add.s64 	%rd561, %rd559, %rd548;
	xor.b64  	%rd562, %rd560, %rd526;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r521,%dummy}, %rd562;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r522}, %rd562;
	}
	shf.r.wrap.b32 	%r523, %r522, %r521, 24;
	shf.r.wrap.b32 	%r524, %r521, %r522, 24;
	mov.b64 	%rd563, {%r524, %r523};
	xor.b64  	%rd564, %rd561, %rd550;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r525,%dummy}, %rd564;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r526}, %rd564;
	}
	shf.r.wrap.b32 	%r527, %r526, %r525, 24;
	shf.r.wrap.b32 	%r528, %r525, %r526, 24;
	mov.b64 	%rd565, {%r528, %r527};
	add.s64 	%rd566, %rd563, %rd554;
	add.s64 	%rd567, %rd565, %rd555;
	xor.b64  	%rd568, %rd566, %rd557;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r529,%dummy}, %rd568;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r530}, %rd568;
	}
	shf.r.wrap.b32 	%r531, %r530, %r529, 16;
	shf.r.wrap.b32 	%r532, %r529, %r530, 16;
	mov.b64 	%rd569, {%r532, %r531};
	xor.b64  	%rd570, %rd567, %rd559;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r533,%dummy}, %rd570;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r534}, %rd570;
	}
	shf.r.wrap.b32 	%r535, %r534, %r533, 16;
	shf.r.wrap.b32 	%r536, %r533, %r534, 16;
	mov.b64 	%rd571, {%r536, %r535};
	add.s64 	%rd572, %rd569, %rd560;
	add.s64 	%rd573, %rd571, %rd561;
	xor.b64  	%rd574, %rd572, %rd563;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r537}, %rd574;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r538,%dummy}, %rd574;
	}
	shf.l.wrap.b32 	%r539, %r538, %r537, 1;
	shf.l.wrap.b32 	%r540, %r537, %r538, 1;
	mov.b64 	%rd575, {%r540, %r539};
	xor.b64  	%rd576, %rd573, %rd565;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r541}, %rd576;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r542,%dummy}, %rd576;
	}
	shf.l.wrap.b32 	%r543, %r542, %r541, 1;
	shf.l.wrap.b32 	%r544, %r541, %r542, 1;
	mov.b64 	%rd577, {%r544, %r543};
	add.s64 	%rd578, %rd552, %rd540;
	add.s64 	%rd579, %rd542, %rd22;
	add.s64 	%rd580, %rd579, %rd524;
	xor.b64  	%rd581, %rd578, %rd520;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd581, 32;
	shr.b64 	%rhs, %rd581, 32;
	add.u64 	%rd582, %lhs, %rhs;
	}
	xor.b64  	%rd583, %rd580, %rd544;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd583, 32;
	shr.b64 	%rhs, %rd583, 32;
	add.u64 	%rd584, %lhs, %rhs;
	}
	add.s64 	%rd585, %rd582, %rd521;
	add.s64 	%rd586, %rd584, %rd522;
	xor.b64  	%rd587, %rd585, %rd552;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r545,%dummy}, %rd587;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r546}, %rd587;
	}
	shf.r.wrap.b32 	%r547, %r546, %r545, 24;
	shf.r.wrap.b32 	%r548, %r545, %r546, 24;
	mov.b64 	%rd588, {%r548, %r547};
	xor.b64  	%rd589, %rd586, %rd524;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r549,%dummy}, %rd589;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r550}, %rd589;
	}
	shf.r.wrap.b32 	%r551, %r550, %r549, 24;
	shf.r.wrap.b32 	%r552, %r549, %r550, 24;
	mov.b64 	%rd590, {%r552, %r551};
	add.s64 	%rd591, %rd588, %rd578;
	add.s64 	%rd592, %rd590, %rd580;
	xor.b64  	%rd593, %rd591, %rd582;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r553,%dummy}, %rd593;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r554}, %rd593;
	}
	shf.r.wrap.b32 	%r555, %r554, %r553, 16;
	shf.r.wrap.b32 	%r556, %r553, %r554, 16;
	mov.b64 	%rd594, {%r556, %r555};
	xor.b64  	%rd595, %rd592, %rd584;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r557,%dummy}, %rd595;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r558}, %rd595;
	}
	shf.r.wrap.b32 	%r559, %r558, %r557, 16;
	shf.r.wrap.b32 	%r560, %r557, %r558, 16;
	mov.b64 	%rd596, {%r560, %r559};
	add.s64 	%rd597, %rd594, %rd585;
	add.s64 	%rd598, %rd596, %rd586;
	xor.b64  	%rd599, %rd597, %rd588;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r561}, %rd599;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r562,%dummy}, %rd599;
	}
	shf.l.wrap.b32 	%r563, %r562, %r561, 1;
	shf.l.wrap.b32 	%r564, %r561, %r562, 1;
	mov.b64 	%rd600, {%r564, %r563};
	xor.b64  	%rd601, %rd598, %rd590;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r565}, %rd601;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r566,%dummy}, %rd601;
	}
	shf.l.wrap.b32 	%r567, %r566, %r565, 1;
	shf.l.wrap.b32 	%r568, %r565, %r566, 1;
	mov.b64 	%rd602, {%r568, %r567};
	add.s64 	%rd603, %rd602, %rd566;
	add.s64 	%rd604, %rd567, %rd22;
	add.s64 	%rd605, %rd604, %rd575;
	xor.b64  	%rd606, %rd603, %rd571;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd606, 32;
	shr.b64 	%rhs, %rd606, 32;
	add.u64 	%rd607, %lhs, %rhs;
	}
	xor.b64  	%rd608, %rd605, %rd594;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd608, 32;
	shr.b64 	%rhs, %rd608, 32;
	add.u64 	%rd609, %lhs, %rhs;
	}
	add.s64 	%rd610, %rd607, %rd597;
	add.s64 	%rd611, %rd609, %rd598;
	xor.b64  	%rd612, %rd610, %rd602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r569,%dummy}, %rd612;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r570}, %rd612;
	}
	shf.r.wrap.b32 	%r571, %r570, %r569, 24;
	shf.r.wrap.b32 	%r572, %r569, %r570, 24;
	mov.b64 	%rd613, {%r572, %r571};
	xor.b64  	%rd614, %rd611, %rd575;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r573,%dummy}, %rd614;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r574}, %rd614;
	}
	shf.r.wrap.b32 	%r575, %r574, %r573, 24;
	shf.r.wrap.b32 	%r576, %r573, %r574, 24;
	mov.b64 	%rd615, {%r576, %r575};
	add.s64 	%rd616, %rd613, %rd603;
	add.s64 	%rd617, %rd615, %rd605;
	xor.b64  	%rd618, %rd616, %rd607;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r577,%dummy}, %rd618;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r578}, %rd618;
	}
	shf.r.wrap.b32 	%r579, %r578, %r577, 16;
	shf.r.wrap.b32 	%r580, %r577, %r578, 16;
	mov.b64 	%rd619, {%r580, %r579};
	xor.b64  	%rd620, %rd617, %rd609;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r581,%dummy}, %rd620;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r582}, %rd620;
	}
	shf.r.wrap.b32 	%r583, %r582, %r581, 16;
	shf.r.wrap.b32 	%r584, %r581, %r582, 16;
	mov.b64 	%rd621, {%r584, %r583};
	add.s64 	%rd622, %rd619, %rd610;
	add.s64 	%rd623, %rd621, %rd611;
	xor.b64  	%rd624, %rd622, %rd613;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r585}, %rd624;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r586,%dummy}, %rd624;
	}
	shf.l.wrap.b32 	%r587, %r586, %r585, 1;
	shf.l.wrap.b32 	%r588, %r585, %r586, 1;
	mov.b64 	%rd625, {%r588, %r587};
	xor.b64  	%rd626, %rd623, %rd615;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r589}, %rd626;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r590,%dummy}, %rd626;
	}
	shf.l.wrap.b32 	%r591, %r590, %r589, 1;
	shf.l.wrap.b32 	%r592, %r589, %r590, 1;
	mov.b64 	%rd627, {%r592, %r591};
	add.s64 	%rd628, %rd577, %rd591;
	add.s64 	%rd629, %rd592, %rd37;
	add.s64 	%rd630, %rd629, %rd600;
	xor.b64  	%rd631, %rd628, %rd596;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd631, 32;
	shr.b64 	%rhs, %rd631, 32;
	add.u64 	%rd632, %lhs, %rhs;
	}
	xor.b64  	%rd633, %rd630, %rd569;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd633, 32;
	shr.b64 	%rhs, %rd633, 32;
	add.u64 	%rd634, %lhs, %rhs;
	}
	add.s64 	%rd635, %rd632, %rd572;
	add.s64 	%rd636, %rd634, %rd573;
	xor.b64  	%rd637, %rd635, %rd577;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r593,%dummy}, %rd637;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r594}, %rd637;
	}
	shf.r.wrap.b32 	%r595, %r594, %r593, 24;
	shf.r.wrap.b32 	%r596, %r593, %r594, 24;
	mov.b64 	%rd638, {%r596, %r595};
	xor.b64  	%rd639, %rd636, %rd600;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r597,%dummy}, %rd639;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r598}, %rd639;
	}
	shf.r.wrap.b32 	%r599, %r598, %r597, 24;
	shf.r.wrap.b32 	%r600, %r597, %r598, 24;
	mov.b64 	%rd640, {%r600, %r599};
	add.s64 	%rd641, %rd638, %rd628;
	add.s64 	%rd642, %rd640, %rd630;
	xor.b64  	%rd643, %rd641, %rd632;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r601,%dummy}, %rd643;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r602}, %rd643;
	}
	shf.r.wrap.b32 	%r603, %r602, %r601, 16;
	shf.r.wrap.b32 	%r604, %r601, %r602, 16;
	mov.b64 	%rd644, {%r604, %r603};
	xor.b64  	%rd645, %rd642, %rd634;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r605,%dummy}, %rd645;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r606}, %rd645;
	}
	shf.r.wrap.b32 	%r607, %r606, %r605, 16;
	shf.r.wrap.b32 	%r608, %r605, %r606, 16;
	mov.b64 	%rd646, {%r608, %r607};
	add.s64 	%rd647, %rd644, %rd635;
	add.s64 	%rd648, %rd646, %rd636;
	xor.b64  	%rd649, %rd647, %rd638;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r609}, %rd649;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r610,%dummy}, %rd649;
	}
	shf.l.wrap.b32 	%r611, %r610, %r609, 1;
	shf.l.wrap.b32 	%r612, %r609, %r610, 1;
	mov.b64 	%rd650, {%r612, %r611};
	xor.b64  	%rd651, %rd648, %rd640;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r613}, %rd651;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r614,%dummy}, %rd651;
	}
	shf.l.wrap.b32 	%r615, %r614, %r613, 1;
	shf.l.wrap.b32 	%r616, %r613, %r614, 1;
	mov.b64 	%rd652, {%r616, %r615};
	add.s64 	%rd653, %rd616, %rd1;
	add.s64 	%rd654, %rd653, %rd627;
	add.s64 	%rd655, %rd650, %rd617;
	xor.b64  	%rd656, %rd654, %rd646;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd656, 32;
	shr.b64 	%rhs, %rd656, 32;
	add.u64 	%rd657, %lhs, %rhs;
	}
	xor.b64  	%rd658, %rd655, %rd619;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd658, 32;
	shr.b64 	%rhs, %rd658, 32;
	add.u64 	%rd659, %lhs, %rhs;
	}
	add.s64 	%rd660, %rd657, %rd647;
	add.s64 	%rd661, %rd659, %rd648;
	xor.b64  	%rd662, %rd660, %rd627;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r617,%dummy}, %rd662;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r618}, %rd662;
	}
	shf.r.wrap.b32 	%r619, %r618, %r617, 24;
	shf.r.wrap.b32 	%r620, %r617, %r618, 24;
	mov.b64 	%rd663, {%r620, %r619};
	xor.b64  	%rd664, %rd661, %rd650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r621,%dummy}, %rd664;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r622}, %rd664;
	}
	shf.r.wrap.b32 	%r623, %r622, %r621, 24;
	shf.r.wrap.b32 	%r624, %r621, %r622, 24;
	mov.b64 	%rd665, {%r624, %r623};
	add.s64 	%rd666, %rd663, %rd654;
	add.s64 	%rd667, %rd655, %rd24;
	add.s64 	%rd668, %rd667, %rd665;
	xor.b64  	%rd669, %rd666, %rd657;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r625,%dummy}, %rd669;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r626}, %rd669;
	}
	shf.r.wrap.b32 	%r627, %r626, %r625, 16;
	shf.r.wrap.b32 	%r628, %r625, %r626, 16;
	mov.b64 	%rd670, {%r628, %r627};
	xor.b64  	%rd671, %rd668, %rd659;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r629,%dummy}, %rd671;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r630}, %rd671;
	}
	shf.r.wrap.b32 	%r631, %r630, %r629, 16;
	shf.r.wrap.b32 	%r632, %r629, %r630, 16;
	mov.b64 	%rd672, {%r632, %r631};
	add.s64 	%rd673, %rd670, %rd660;
	add.s64 	%rd674, %rd672, %rd661;
	xor.b64  	%rd675, %rd673, %rd663;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r633}, %rd675;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r634,%dummy}, %rd675;
	}
	shf.l.wrap.b32 	%r635, %r634, %r633, 1;
	shf.l.wrap.b32 	%r636, %r633, %r634, 1;
	mov.b64 	%rd676, {%r636, %r635};
	xor.b64  	%rd677, %rd674, %rd665;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r637}, %rd677;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r638,%dummy}, %rd677;
	}
	shf.l.wrap.b32 	%r639, %r638, %r637, 1;
	shf.l.wrap.b32 	%r640, %r637, %r638, 1;
	mov.b64 	%rd678, {%r640, %r639};
	add.s64 	%rd679, %rd652, %rd641;
	add.s64 	%rd680, %rd625, %rd642;
	xor.b64  	%rd681, %rd621, %rd679;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd681, 32;
	shr.b64 	%rhs, %rd681, 32;
	add.u64 	%rd682, %lhs, %rhs;
	}
	xor.b64  	%rd683, %rd680, %rd644;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd683, 32;
	shr.b64 	%rhs, %rd683, 32;
	add.u64 	%rd684, %lhs, %rhs;
	}
	add.s64 	%rd685, %rd682, %rd622;
	add.s64 	%rd686, %rd684, %rd623;
	xor.b64  	%rd687, %rd685, %rd652;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r641,%dummy}, %rd687;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r642}, %rd687;
	}
	shf.r.wrap.b32 	%r643, %r642, %r641, 24;
	shf.r.wrap.b32 	%r644, %r641, %r642, 24;
	mov.b64 	%rd688, {%r644, %r643};
	xor.b64  	%rd689, %rd686, %rd625;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r645,%dummy}, %rd689;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r646}, %rd689;
	}
	shf.r.wrap.b32 	%r647, %r646, %r645, 24;
	shf.r.wrap.b32 	%r648, %r645, %r646, 24;
	mov.b64 	%rd690, {%r648, %r647};
	add.s64 	%rd691, %rd679, %rd9;
	add.s64 	%rd692, %rd691, %rd688;
	add.s64 	%rd693, %rd690, %rd680;
	xor.b64  	%rd694, %rd692, %rd682;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r649,%dummy}, %rd694;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r650}, %rd694;
	}
	shf.r.wrap.b32 	%r651, %r650, %r649, 16;
	shf.r.wrap.b32 	%r652, %r649, %r650, 16;
	mov.b64 	%rd695, {%r652, %r651};
	xor.b64  	%rd696, %rd693, %rd684;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r653,%dummy}, %rd696;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r654}, %rd696;
	}
	shf.r.wrap.b32 	%r655, %r654, %r653, 16;
	shf.r.wrap.b32 	%r656, %r653, %r654, 16;
	mov.b64 	%rd697, {%r656, %r655};
	add.s64 	%rd698, %rd695, %rd685;
	add.s64 	%rd699, %rd697, %rd686;
	xor.b64  	%rd700, %rd698, %rd688;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r657}, %rd700;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r658,%dummy}, %rd700;
	}
	shf.l.wrap.b32 	%r659, %r658, %r657, 1;
	shf.l.wrap.b32 	%r660, %r657, %r658, 1;
	mov.b64 	%rd701, {%r660, %r659};
	xor.b64  	%rd702, %rd699, %rd690;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r661}, %rd702;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r662,%dummy}, %rd702;
	}
	shf.l.wrap.b32 	%r663, %r662, %r661, 1;
	shf.l.wrap.b32 	%r664, %r661, %r662, 1;
	mov.b64 	%rd703, {%r664, %r663};
	add.s64 	%rd704, %rd703, %rd666;
	add.s64 	%rd705, %rd676, %rd668;
	xor.b64  	%rd706, %rd672, %rd704;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd706, 32;
	shr.b64 	%rhs, %rd706, 32;
	add.u64 	%rd707, %lhs, %rhs;
	}
	xor.b64  	%rd708, %rd705, %rd695;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd708, 32;
	shr.b64 	%rhs, %rd708, 32;
	add.u64 	%rd709, %lhs, %rhs;
	}
	add.s64 	%rd710, %rd707, %rd698;
	add.s64 	%rd711, %rd709, %rd699;
	xor.b64  	%rd712, %rd710, %rd703;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r665,%dummy}, %rd712;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r666}, %rd712;
	}
	shf.r.wrap.b32 	%r667, %r666, %r665, 24;
	shf.r.wrap.b32 	%r668, %r665, %r666, 24;
	mov.b64 	%rd713, {%r668, %r667};
	xor.b64  	%rd714, %rd711, %rd676;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r669,%dummy}, %rd714;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r670}, %rd714;
	}
	shf.r.wrap.b32 	%r671, %r670, %r669, 24;
	shf.r.wrap.b32 	%r672, %r669, %r670, 24;
	mov.b64 	%rd715, {%r672, %r671};
	add.s64 	%rd716, %rd713, %rd704;
	add.s64 	%rd717, %rd715, %rd705;
	xor.b64  	%rd718, %rd716, %rd707;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r673,%dummy}, %rd718;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r674}, %rd718;
	}
	shf.r.wrap.b32 	%r675, %r674, %r673, 16;
	shf.r.wrap.b32 	%r676, %r673, %r674, 16;
	mov.b64 	%rd719, {%r676, %r675};
	xor.b64  	%rd720, %rd717, %rd709;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r677,%dummy}, %rd720;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r678}, %rd720;
	}
	shf.r.wrap.b32 	%r679, %r678, %r677, 16;
	shf.r.wrap.b32 	%r680, %r677, %r678, 16;
	mov.b64 	%rd721, {%r680, %r679};
	add.s64 	%rd722, %rd719, %rd710;
	add.s64 	%rd723, %rd721, %rd711;
	xor.b64  	%rd724, %rd722, %rd713;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r681}, %rd724;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r682,%dummy}, %rd724;
	}
	shf.l.wrap.b32 	%r683, %r682, %r681, 1;
	shf.l.wrap.b32 	%r684, %r681, %r682, 1;
	mov.b64 	%rd725, {%r684, %r683};
	xor.b64  	%rd726, %rd723, %rd715;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r685}, %rd726;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r686,%dummy}, %rd726;
	}
	shf.l.wrap.b32 	%r687, %r686, %r685, 1;
	shf.l.wrap.b32 	%r688, %r685, %r686, 1;
	mov.b64 	%rd727, {%r688, %r687};
	add.s64 	%rd728, %rd678, %rd692;
	add.s64 	%rd729, %rd693, %rd24;
	add.s64 	%rd730, %rd729, %rd701;
	xor.b64  	%rd731, %rd728, %rd697;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd731, 32;
	shr.b64 	%rhs, %rd731, 32;
	add.u64 	%rd732, %lhs, %rhs;
	}
	xor.b64  	%rd733, %rd730, %rd670;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd733, 32;
	shr.b64 	%rhs, %rd733, 32;
	add.u64 	%rd734, %lhs, %rhs;
	}
	add.s64 	%rd735, %rd732, %rd673;
	add.s64 	%rd736, %rd734, %rd674;
	xor.b64  	%rd737, %rd735, %rd678;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r689,%dummy}, %rd737;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r690}, %rd737;
	}
	shf.r.wrap.b32 	%r691, %r690, %r689, 24;
	shf.r.wrap.b32 	%r692, %r689, %r690, 24;
	mov.b64 	%rd738, {%r692, %r691};
	xor.b64  	%rd739, %rd736, %rd701;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r693,%dummy}, %rd739;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r694}, %rd739;
	}
	shf.r.wrap.b32 	%r695, %r694, %r693, 24;
	shf.r.wrap.b32 	%r696, %r693, %r694, 24;
	mov.b64 	%rd740, {%r696, %r695};
	add.s64 	%rd741, %rd728, %rd22;
	add.s64 	%rd742, %rd741, %rd738;
	add.s64 	%rd743, %rd740, %rd730;
	xor.b64  	%rd744, %rd742, %rd732;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r697,%dummy}, %rd744;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r698}, %rd744;
	}
	shf.r.wrap.b32 	%r699, %r698, %r697, 16;
	shf.r.wrap.b32 	%r700, %r697, %r698, 16;
	mov.b64 	%rd745, {%r700, %r699};
	xor.b64  	%rd746, %rd743, %rd734;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r701,%dummy}, %rd746;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r702}, %rd746;
	}
	shf.r.wrap.b32 	%r703, %r702, %r701, 16;
	shf.r.wrap.b32 	%r704, %r701, %r702, 16;
	mov.b64 	%rd747, {%r704, %r703};
	add.s64 	%rd748, %rd745, %rd735;
	add.s64 	%rd749, %rd747, %rd736;
	xor.b64  	%rd750, %rd748, %rd738;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r705}, %rd750;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r706,%dummy}, %rd750;
	}
	shf.l.wrap.b32 	%r707, %r706, %r705, 1;
	shf.l.wrap.b32 	%r708, %r705, %r706, 1;
	mov.b64 	%rd751, {%r708, %r707};
	xor.b64  	%rd752, %rd749, %rd740;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r709}, %rd752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r710,%dummy}, %rd752;
	}
	shf.l.wrap.b32 	%r711, %r710, %r709, 1;
	shf.l.wrap.b32 	%r712, %r709, %r710, 1;
	mov.b64 	%rd753, {%r712, %r711};
	add.s64 	%rd754, %rd727, %rd716;
	add.s64 	%rd755, %rd751, %rd717;
	xor.b64  	%rd756, %rd754, %rd747;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd756, 32;
	shr.b64 	%rhs, %rd756, 32;
	add.u64 	%rd757, %lhs, %rhs;
	}
	xor.b64  	%rd758, %rd755, %rd719;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd758, 32;
	shr.b64 	%rhs, %rd758, 32;
	add.u64 	%rd759, %lhs, %rhs;
	}
	add.s64 	%rd760, %rd757, %rd748;
	add.s64 	%rd761, %rd759, %rd749;
	xor.b64  	%rd762, %rd760, %rd727;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r713,%dummy}, %rd762;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r714}, %rd762;
	}
	shf.r.wrap.b32 	%r715, %r714, %r713, 24;
	shf.r.wrap.b32 	%r716, %r713, %r714, 24;
	mov.b64 	%rd763, {%r716, %r715};
	xor.b64  	%rd764, %rd761, %rd751;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r717,%dummy}, %rd764;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r718}, %rd764;
	}
	shf.r.wrap.b32 	%r719, %r718, %r717, 24;
	shf.r.wrap.b32 	%r720, %r717, %r718, 24;
	mov.b64 	%rd765, {%r720, %r719};
	add.s64 	%rd766, %rd754, %rd1;
	add.s64 	%rd767, %rd766, %rd763;
	add.s64 	%rd768, %rd755, %rd37;
	add.s64 	%rd769, %rd768, %rd765;
	xor.b64  	%rd770, %rd767, %rd757;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r721,%dummy}, %rd770;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r722}, %rd770;
	}
	shf.r.wrap.b32 	%r723, %r722, %r721, 16;
	shf.r.wrap.b32 	%r724, %r721, %r722, 16;
	mov.b64 	%rd771, {%r724, %r723};
	xor.b64  	%rd772, %rd769, %rd759;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r725,%dummy}, %rd772;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r726}, %rd772;
	}
	shf.r.wrap.b32 	%r727, %r726, %r725, 16;
	shf.r.wrap.b32 	%r728, %r725, %r726, 16;
	mov.b64 	%rd773, {%r728, %r727};
	add.s64 	%rd774, %rd771, %rd760;
	add.s64 	%rd775, %rd773, %rd761;
	xor.b64  	%rd776, %rd774, %rd763;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r729}, %rd776;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r730,%dummy}, %rd776;
	}
	shf.l.wrap.b32 	%r731, %r730, %r729, 1;
	shf.l.wrap.b32 	%r732, %r729, %r730, 1;
	mov.b64 	%rd777, {%r732, %r731};
	xor.b64  	%rd778, %rd775, %rd765;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r733}, %rd778;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r734,%dummy}, %rd778;
	}
	shf.l.wrap.b32 	%r735, %r734, %r733, 1;
	shf.l.wrap.b32 	%r736, %r733, %r734, 1;
	mov.b64 	%rd779, {%r736, %r735};
	add.s64 	%rd780, %rd753, %rd742;
	add.s64 	%rd781, %rd743, %rd9;
	add.s64 	%rd782, %rd781, %rd725;
	xor.b64  	%rd783, %rd780, %rd721;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd783, 32;
	shr.b64 	%rhs, %rd783, 32;
	add.u64 	%rd784, %lhs, %rhs;
	}
	xor.b64  	%rd785, %rd745, %rd782;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd785, 32;
	shr.b64 	%rhs, %rd785, 32;
	add.u64 	%rd786, %lhs, %rhs;
	}
	add.s64 	%rd787, %rd784, %rd722;
	add.s64 	%rd788, %rd786, %rd723;
	xor.b64  	%rd789, %rd787, %rd753;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r737,%dummy}, %rd789;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r738}, %rd789;
	}
	shf.r.wrap.b32 	%r739, %r738, %r737, 24;
	shf.r.wrap.b32 	%r740, %r737, %r738, 24;
	mov.b64 	%rd790, {%r740, %r739};
	xor.b64  	%rd791, %rd788, %rd725;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r741,%dummy}, %rd791;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r742}, %rd791;
	}
	shf.r.wrap.b32 	%r743, %r742, %r741, 24;
	shf.r.wrap.b32 	%r744, %r741, %r742, 24;
	mov.b64 	%rd792, {%r744, %r743};
	add.s64 	%rd793, %rd790, %rd780;
	add.s64 	%rd794, %rd792, %rd782;
	xor.b64  	%rd795, %rd793, %rd784;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r745,%dummy}, %rd795;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r746}, %rd795;
	}
	shf.r.wrap.b32 	%r747, %r746, %r745, 16;
	shf.r.wrap.b32 	%r748, %r745, %r746, 16;
	mov.b64 	%rd796, {%r748, %r747};
	xor.b64  	%rd797, %rd794, %rd786;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r749,%dummy}, %rd797;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r750}, %rd797;
	}
	shf.r.wrap.b32 	%r751, %r750, %r749, 16;
	shf.r.wrap.b32 	%r752, %r749, %r750, 16;
	mov.b64 	%rd798, {%r752, %r751};
	add.s64 	%rd799, %rd796, %rd787;
	add.s64 	%rd800, %rd798, %rd788;
	xor.b64  	%rd801, %rd799, %rd790;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r753}, %rd801;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r754,%dummy}, %rd801;
	}
	shf.l.wrap.b32 	%r755, %r754, %r753, 1;
	shf.l.wrap.b32 	%r756, %r753, %r754, 1;
	mov.b64 	%rd802, {%r756, %r755};
	xor.b64  	%rd803, %rd800, %rd792;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r757}, %rd803;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r758,%dummy}, %rd803;
	}
	shf.l.wrap.b32 	%r759, %r758, %r757, 1;
	shf.l.wrap.b32 	%r760, %r757, %r758, 1;
	mov.b64 	%rd804, {%r760, %r759};
	add.s64 	%rd805, %rd804, %rd767;
	add.s64 	%rd806, %rd777, %rd769;
	xor.b64  	%rd807, %rd773, %rd805;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd807, 32;
	shr.b64 	%rhs, %rd807, 32;
	add.u64 	%rd808, %lhs, %rhs;
	}
	xor.b64  	%rd809, %rd806, %rd796;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd809, 32;
	shr.b64 	%rhs, %rd809, 32;
	add.u64 	%rd810, %lhs, %rhs;
	}
	add.s64 	%rd811, %rd808, %rd799;
	add.s64 	%rd812, %rd810, %rd800;
	xor.b64  	%rd813, %rd811, %rd804;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r761,%dummy}, %rd813;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r762}, %rd813;
	}
	shf.r.wrap.b32 	%r763, %r762, %r761, 24;
	shf.r.wrap.b32 	%r764, %r761, %r762, 24;
	mov.b64 	%rd814, {%r764, %r763};
	xor.b64  	%rd815, %rd812, %rd777;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r765,%dummy}, %rd815;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r766}, %rd815;
	}
	shf.r.wrap.b32 	%r767, %r766, %r765, 24;
	shf.r.wrap.b32 	%r768, %r765, %r766, 24;
	mov.b64 	%rd816, {%r768, %r767};
	add.s64 	%rd817, %rd814, %rd805;
	add.s64 	%rd818, %rd816, %rd806;
	xor.b64  	%rd819, %rd817, %rd808;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r769,%dummy}, %rd819;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r770}, %rd819;
	}
	shf.r.wrap.b32 	%r771, %r770, %r769, 16;
	shf.r.wrap.b32 	%r772, %r769, %r770, 16;
	mov.b64 	%rd820, {%r772, %r771};
	xor.b64  	%rd821, %rd818, %rd810;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r773,%dummy}, %rd821;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r774}, %rd821;
	}
	shf.r.wrap.b32 	%r775, %r774, %r773, 16;
	shf.r.wrap.b32 	%r776, %r773, %r774, 16;
	mov.b64 	%rd822, {%r776, %r775};
	add.s64 	%rd823, %rd820, %rd811;
	add.s64 	%rd824, %rd822, %rd812;
	xor.b64  	%rd825, %rd823, %rd814;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r777}, %rd825;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r778,%dummy}, %rd825;
	}
	shf.l.wrap.b32 	%r779, %r778, %r777, 1;
	shf.l.wrap.b32 	%r780, %r777, %r778, 1;
	mov.b64 	%rd826, {%r780, %r779};
	xor.b64  	%rd827, %rd824, %rd816;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r781}, %rd827;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r782,%dummy}, %rd827;
	}
	shf.l.wrap.b32 	%r783, %r782, %r781, 1;
	shf.l.wrap.b32 	%r784, %r781, %r782, 1;
	mov.b64 	%rd828, {%r784, %r783};
	add.s64 	%rd829, %rd779, %rd793;
	add.s64 	%rd830, %rd794, %rd1;
	add.s64 	%rd831, %rd830, %rd802;
	xor.b64  	%rd832, %rd829, %rd798;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd832, 32;
	shr.b64 	%rhs, %rd832, 32;
	add.u64 	%rd833, %lhs, %rhs;
	}
	xor.b64  	%rd834, %rd831, %rd771;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd834, 32;
	shr.b64 	%rhs, %rd834, 32;
	add.u64 	%rd835, %lhs, %rhs;
	}
	add.s64 	%rd836, %rd833, %rd774;
	add.s64 	%rd837, %rd835, %rd775;
	xor.b64  	%rd838, %rd836, %rd779;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r785,%dummy}, %rd838;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r786}, %rd838;
	}
	shf.r.wrap.b32 	%r787, %r786, %r785, 24;
	shf.r.wrap.b32 	%r788, %r785, %r786, 24;
	mov.b64 	%rd839, {%r788, %r787};
	xor.b64  	%rd840, %rd837, %rd802;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r789,%dummy}, %rd840;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r790}, %rd840;
	}
	shf.r.wrap.b32 	%r791, %r790, %r789, 24;
	shf.r.wrap.b32 	%r792, %r789, %r790, 24;
	mov.b64 	%rd841, {%r792, %r791};
	add.s64 	%rd842, %rd829, %rd24;
	add.s64 	%rd843, %rd842, %rd839;
	add.s64 	%rd844, %rd841, %rd831;
	xor.b64  	%rd845, %rd843, %rd833;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r793,%dummy}, %rd845;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r794}, %rd845;
	}
	shf.r.wrap.b32 	%r795, %r794, %r793, 16;
	shf.r.wrap.b32 	%r796, %r793, %r794, 16;
	mov.b64 	%rd846, {%r796, %r795};
	xor.b64  	%rd847, %rd844, %rd835;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r797,%dummy}, %rd847;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r798}, %rd847;
	}
	shf.r.wrap.b32 	%r799, %r798, %r797, 16;
	shf.r.wrap.b32 	%r800, %r797, %r798, 16;
	mov.b64 	%rd848, {%r800, %r799};
	add.s64 	%rd849, %rd846, %rd836;
	add.s64 	%rd850, %rd848, %rd837;
	xor.b64  	%rd851, %rd849, %rd839;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r801}, %rd851;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r802,%dummy}, %rd851;
	}
	shf.l.wrap.b32 	%r803, %r802, %r801, 1;
	shf.l.wrap.b32 	%r804, %r801, %r802, 1;
	mov.b64 	%rd852, {%r804, %r803};
	xor.b64  	%rd853, %rd850, %rd841;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r805}, %rd853;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r806,%dummy}, %rd853;
	}
	shf.l.wrap.b32 	%r807, %r806, %r805, 1;
	shf.l.wrap.b32 	%r808, %r805, %r806, 1;
	mov.b64 	%rd854, {%r808, %r807};
	add.s64 	%rd855, %rd828, %rd817;
	add.s64 	%rd856, %rd852, %rd818;
	xor.b64  	%rd857, %rd855, %rd848;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd857, 32;
	shr.b64 	%rhs, %rd857, 32;
	add.u64 	%rd858, %lhs, %rhs;
	}
	xor.b64  	%rd859, %rd856, %rd820;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd859, 32;
	shr.b64 	%rhs, %rd859, 32;
	add.u64 	%rd860, %lhs, %rhs;
	}
	add.s64 	%rd861, %rd858, %rd849;
	add.s64 	%rd862, %rd860, %rd850;
	xor.b64  	%rd863, %rd861, %rd828;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r809,%dummy}, %rd863;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r810}, %rd863;
	}
	shf.r.wrap.b32 	%r811, %r810, %r809, 24;
	shf.r.wrap.b32 	%r812, %r809, %r810, 24;
	mov.b64 	%rd864, {%r812, %r811};
	xor.b64  	%rd865, %rd862, %rd852;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r813,%dummy}, %rd865;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r814}, %rd865;
	}
	shf.r.wrap.b32 	%r815, %r814, %r813, 24;
	shf.r.wrap.b32 	%r816, %r813, %r814, 24;
	mov.b64 	%rd866, {%r816, %r815};
	add.s64 	%rd867, %rd855, %rd9;
	add.s64 	%rd868, %rd867, %rd864;
	add.s64 	%rd869, %rd866, %rd856;
	xor.b64  	%rd870, %rd868, %rd858;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r817,%dummy}, %rd870;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r818}, %rd870;
	}
	shf.r.wrap.b32 	%r819, %r818, %r817, 16;
	shf.r.wrap.b32 	%r820, %r817, %r818, 16;
	mov.b64 	%rd871, {%r820, %r819};
	xor.b64  	%rd872, %rd869, %rd860;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r821,%dummy}, %rd872;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r822}, %rd872;
	}
	shf.r.wrap.b32 	%r823, %r822, %r821, 16;
	shf.r.wrap.b32 	%r824, %r821, %r822, 16;
	mov.b64 	%rd873, {%r824, %r823};
	add.s64 	%rd874, %rd871, %rd861;
	add.s64 	%rd875, %rd873, %rd862;
	xor.b64  	%rd876, %rd874, %rd864;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r825}, %rd876;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r826,%dummy}, %rd876;
	}
	shf.l.wrap.b32 	%r827, %r826, %r825, 1;
	shf.l.wrap.b32 	%r828, %r825, %r826, 1;
	mov.b64 	%rd877, {%r828, %r827};
	xor.b64  	%rd878, %rd875, %rd866;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r829}, %rd878;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r830,%dummy}, %rd878;
	}
	shf.l.wrap.b32 	%r831, %r830, %r829, 1;
	shf.l.wrap.b32 	%r832, %r829, %r830, 1;
	mov.b64 	%rd879, {%r832, %r831};
	add.s64 	%rd880, %rd843, %rd22;
	add.s64 	%rd881, %rd880, %rd854;
	add.s64 	%rd882, %rd826, %rd844;
	xor.b64  	%rd883, %rd881, %rd822;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd883, 32;
	shr.b64 	%rhs, %rd883, 32;
	add.u64 	%rd884, %lhs, %rhs;
	}
	xor.b64  	%rd885, %rd846, %rd882;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd885, 32;
	shr.b64 	%rhs, %rd885, 32;
	add.u64 	%rd886, %lhs, %rhs;
	}
	add.s64 	%rd887, %rd884, %rd823;
	add.s64 	%rd888, %rd886, %rd824;
	xor.b64  	%rd889, %rd887, %rd854;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r833,%dummy}, %rd889;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r834}, %rd889;
	}
	shf.r.wrap.b32 	%r835, %r834, %r833, 24;
	shf.r.wrap.b32 	%r836, %r833, %r834, 24;
	mov.b64 	%rd890, {%r836, %r835};
	xor.b64  	%rd891, %rd888, %rd826;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r837,%dummy}, %rd891;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r838}, %rd891;
	}
	shf.r.wrap.b32 	%r839, %r838, %r837, 24;
	shf.r.wrap.b32 	%r840, %r837, %r838, 24;
	mov.b64 	%rd892, {%r840, %r839};
	add.s64 	%rd893, %rd881, %rd37;
	add.s64 	%rd894, %rd893, %rd890;
	add.s64 	%rd895, %rd892, %rd882;
	xor.b64  	%rd896, %rd894, %rd884;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r841,%dummy}, %rd896;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r842}, %rd896;
	}
	shf.r.wrap.b32 	%r843, %r842, %r841, 16;
	shf.r.wrap.b32 	%r844, %r841, %r842, 16;
	mov.b64 	%rd897, {%r844, %r843};
	xor.b64  	%rd898, %rd895, %rd886;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r845,%dummy}, %rd898;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r846}, %rd898;
	}
	shf.r.wrap.b32 	%r847, %r846, %r845, 16;
	shf.r.wrap.b32 	%r848, %r845, %r846, 16;
	mov.b64 	%rd899, {%r848, %r847};
	add.s64 	%rd900, %rd897, %rd887;
	add.s64 	%rd901, %rd899, %rd888;
	xor.b64  	%rd902, %rd900, %rd890;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r849}, %rd902;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r850,%dummy}, %rd902;
	}
	shf.l.wrap.b32 	%r851, %r850, %r849, 1;
	shf.l.wrap.b32 	%r852, %r849, %r850, 1;
	mov.b64 	%rd903, {%r852, %r851};
	xor.b64  	%rd904, %rd901, %rd892;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r853}, %rd904;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r854,%dummy}, %rd904;
	}
	shf.l.wrap.b32 	%r855, %r854, %r853, 1;
	shf.l.wrap.b32 	%r856, %r853, %r854, 1;
	mov.b64 	%rd905, {%r856, %r855};
	add.s64 	%rd906, %rd905, %rd868;
	add.s64 	%rd907, %rd877, %rd869;
	xor.b64  	%rd908, %rd873, %rd906;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd908, 32;
	shr.b64 	%rhs, %rd908, 32;
	add.u64 	%rd909, %lhs, %rhs;
	}
	xor.b64  	%rd910, %rd907, %rd897;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd910, 32;
	shr.b64 	%rhs, %rd910, 32;
	add.u64 	%rd911, %lhs, %rhs;
	}
	add.s64 	%rd912, %rd909, %rd900;
	add.s64 	%rd913, %rd911, %rd901;
	xor.b64  	%rd914, %rd912, %rd905;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r857,%dummy}, %rd914;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r858}, %rd914;
	}
	shf.r.wrap.b32 	%r859, %r858, %r857, 24;
	shf.r.wrap.b32 	%r860, %r857, %r858, 24;
	mov.b64 	%rd915, {%r860, %r859};
	xor.b64  	%rd916, %rd913, %rd877;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r861,%dummy}, %rd916;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r862}, %rd916;
	}
	shf.r.wrap.b32 	%r863, %r862, %r861, 24;
	shf.r.wrap.b32 	%r864, %r861, %r862, 24;
	mov.b64 	%rd917, {%r864, %r863};
	add.s64 	%rd918, %rd906, %rd9;
	add.s64 	%rd919, %rd918, %rd915;
	add.s64 	%rd920, %rd907, %rd37;
	add.s64 	%rd921, %rd920, %rd917;
	xor.b64  	%rd922, %rd919, %rd909;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r865,%dummy}, %rd922;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r866}, %rd922;
	}
	shf.r.wrap.b32 	%r867, %r866, %r865, 16;
	shf.r.wrap.b32 	%r868, %r865, %r866, 16;
	mov.b64 	%rd923, {%r868, %r867};
	xor.b64  	%rd924, %rd921, %rd911;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r869,%dummy}, %rd924;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r870}, %rd924;
	}
	shf.r.wrap.b32 	%r871, %r870, %r869, 16;
	shf.r.wrap.b32 	%r872, %r869, %r870, 16;
	mov.b64 	%rd925, {%r872, %r871};
	add.s64 	%rd926, %rd923, %rd912;
	add.s64 	%rd927, %rd925, %rd913;
	xor.b64  	%rd928, %rd926, %rd915;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r873}, %rd928;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r874,%dummy}, %rd928;
	}
	shf.l.wrap.b32 	%r875, %r874, %r873, 1;
	shf.l.wrap.b32 	%r876, %r873, %r874, 1;
	mov.b64 	%rd929, {%r876, %r875};
	xor.b64  	%rd930, %rd927, %rd917;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r877}, %rd930;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r878,%dummy}, %rd930;
	}
	shf.l.wrap.b32 	%r879, %r878, %r877, 1;
	shf.l.wrap.b32 	%r880, %r877, %r878, 1;
	mov.b64 	%rd931, {%r880, %r879};
	add.s64 	%rd932, %rd879, %rd894;
	add.s64 	%rd933, %rd895, %rd22;
	add.s64 	%rd934, %rd933, %rd903;
	xor.b64  	%rd935, %rd932, %rd899;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd935, 32;
	shr.b64 	%rhs, %rd935, 32;
	add.u64 	%rd936, %lhs, %rhs;
	}
	xor.b64  	%rd937, %rd934, %rd871;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd937, 32;
	shr.b64 	%rhs, %rd937, 32;
	add.u64 	%rd938, %lhs, %rhs;
	}
	add.s64 	%rd939, %rd936, %rd874;
	add.s64 	%rd940, %rd938, %rd875;
	xor.b64  	%rd941, %rd939, %rd879;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r881,%dummy}, %rd941;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r882}, %rd941;
	}
	shf.r.wrap.b32 	%r883, %r882, %r881, 24;
	shf.r.wrap.b32 	%r884, %r881, %r882, 24;
	mov.b64 	%rd942, {%r884, %r883};
	xor.b64  	%rd943, %rd940, %rd903;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r885,%dummy}, %rd943;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r886}, %rd943;
	}
	shf.r.wrap.b32 	%r887, %r886, %r885, 24;
	shf.r.wrap.b32 	%r888, %r885, %r886, 24;
	mov.b64 	%rd944, {%r888, %r887};
	add.s64 	%rd945, %rd942, %rd932;
	add.s64 	%rd946, %rd944, %rd934;
	xor.b64  	%rd947, %rd945, %rd936;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r889,%dummy}, %rd947;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r890}, %rd947;
	}
	shf.r.wrap.b32 	%r891, %r890, %r889, 16;
	shf.r.wrap.b32 	%r892, %r889, %r890, 16;
	mov.b64 	%rd948, {%r892, %r891};
	xor.b64  	%rd949, %rd946, %rd938;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r893,%dummy}, %rd949;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r894}, %rd949;
	}
	shf.r.wrap.b32 	%r895, %r894, %r893, 16;
	shf.r.wrap.b32 	%r896, %r893, %r894, 16;
	mov.b64 	%rd950, {%r896, %r895};
	add.s64 	%rd951, %rd948, %rd939;
	add.s64 	%rd952, %rd950, %rd940;
	xor.b64  	%rd953, %rd951, %rd942;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r897}, %rd953;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r898,%dummy}, %rd953;
	}
	shf.l.wrap.b32 	%r899, %r898, %r897, 1;
	shf.l.wrap.b32 	%r900, %r897, %r898, 1;
	mov.b64 	%rd954, {%r900, %r899};
	xor.b64  	%rd955, %rd952, %rd944;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r901}, %rd955;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r902,%dummy}, %rd955;
	}
	shf.l.wrap.b32 	%r903, %r902, %r901, 1;
	shf.l.wrap.b32 	%r904, %r901, %r902, 1;
	mov.b64 	%rd956, {%r904, %r903};
	add.s64 	%rd957, %rd931, %rd919;
	add.s64 	%rd958, %rd954, %rd921;
	xor.b64  	%rd959, %rd957, %rd950;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd959, 32;
	shr.b64 	%rhs, %rd959, 32;
	add.u64 	%rd960, %lhs, %rhs;
	}
	xor.b64  	%rd961, %rd958, %rd923;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd961, 32;
	shr.b64 	%rhs, %rd961, 32;
	add.u64 	%rd962, %lhs, %rhs;
	}
	add.s64 	%rd963, %rd960, %rd951;
	add.s64 	%rd964, %rd962, %rd952;
	xor.b64  	%rd965, %rd963, %rd931;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r905,%dummy}, %rd965;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r906}, %rd965;
	}
	shf.r.wrap.b32 	%r907, %r906, %r905, 24;
	shf.r.wrap.b32 	%r908, %r905, %r906, 24;
	mov.b64 	%rd966, {%r908, %r907};
	xor.b64  	%rd967, %rd964, %rd954;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r909,%dummy}, %rd967;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r910}, %rd967;
	}
	shf.r.wrap.b32 	%r911, %r910, %r909, 24;
	shf.r.wrap.b32 	%r912, %r909, %r910, 24;
	mov.b64 	%rd968, {%r912, %r911};
	add.s64 	%rd969, %rd966, %rd957;
	add.s64 	%rd970, %rd968, %rd958;
	xor.b64  	%rd971, %rd969, %rd960;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r913,%dummy}, %rd971;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r914}, %rd971;
	}
	shf.r.wrap.b32 	%r915, %r914, %r913, 16;
	shf.r.wrap.b32 	%r916, %r913, %r914, 16;
	mov.b64 	%rd972, {%r916, %r915};
	xor.b64  	%rd973, %rd970, %rd962;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r917,%dummy}, %rd973;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r918}, %rd973;
	}
	shf.r.wrap.b32 	%r919, %r918, %r917, 16;
	shf.r.wrap.b32 	%r920, %r917, %r918, 16;
	mov.b64 	%rd974, {%r920, %r919};
	add.s64 	%rd975, %rd972, %rd963;
	add.s64 	%rd976, %rd974, %rd964;
	xor.b64  	%rd977, %rd975, %rd966;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r921}, %rd977;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r922,%dummy}, %rd977;
	}
	shf.l.wrap.b32 	%r923, %r922, %r921, 1;
	shf.l.wrap.b32 	%r924, %r921, %r922, 1;
	mov.b64 	%rd978, {%r924, %r923};
	xor.b64  	%rd979, %rd976, %rd968;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r925}, %rd979;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r926,%dummy}, %rd979;
	}
	shf.l.wrap.b32 	%r927, %r926, %r925, 1;
	shf.l.wrap.b32 	%r928, %r925, %r926, 1;
	mov.b64 	%rd980, {%r928, %r927};
	add.s64 	%rd981, %rd945, %rd24;
	add.s64 	%rd982, %rd981, %rd956;
	add.s64 	%rd983, %rd929, %rd946;
	xor.b64  	%rd984, %rd982, %rd925;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd984, 32;
	shr.b64 	%rhs, %rd984, 32;
	add.u64 	%rd985, %lhs, %rhs;
	}
	xor.b64  	%rd986, %rd983, %rd948;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd986, 32;
	shr.b64 	%rhs, %rd986, 32;
	add.u64 	%rd987, %lhs, %rhs;
	}
	add.s64 	%rd988, %rd985, %rd926;
	add.s64 	%rd989, %rd987, %rd927;
	xor.b64  	%rd990, %rd988, %rd956;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r929,%dummy}, %rd990;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r930}, %rd990;
	}
	shf.r.wrap.b32 	%r931, %r930, %r929, 24;
	shf.r.wrap.b32 	%r932, %r929, %r930, 24;
	mov.b64 	%rd991, {%r932, %r931};
	xor.b64  	%rd992, %rd989, %rd929;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r933,%dummy}, %rd992;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r934}, %rd992;
	}
	shf.r.wrap.b32 	%r935, %r934, %r933, 24;
	shf.r.wrap.b32 	%r936, %r933, %r934, 24;
	mov.b64 	%rd993, {%r936, %r935};
	add.s64 	%rd994, %rd991, %rd982;
	add.s64 	%rd995, %rd983, %rd1;
	add.s64 	%rd996, %rd995, %rd993;
	xor.b64  	%rd997, %rd994, %rd985;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r937,%dummy}, %rd997;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r938}, %rd997;
	}
	shf.r.wrap.b32 	%r939, %r938, %r937, 16;
	shf.r.wrap.b32 	%r940, %r937, %r938, 16;
	mov.b64 	%rd998, {%r940, %r939};
	xor.b64  	%rd999, %rd996, %rd987;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r941,%dummy}, %rd999;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r942}, %rd999;
	}
	shf.r.wrap.b32 	%r943, %r942, %r941, 16;
	shf.r.wrap.b32 	%r944, %r941, %r942, 16;
	mov.b64 	%rd1000, {%r944, %r943};
	add.s64 	%rd1001, %rd998, %rd988;
	add.s64 	%rd1002, %rd1000, %rd989;
	xor.b64  	%rd1003, %rd1001, %rd991;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r945}, %rd1003;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r946,%dummy}, %rd1003;
	}
	shf.l.wrap.b32 	%r947, %r946, %r945, 1;
	shf.l.wrap.b32 	%r948, %r945, %r946, 1;
	mov.b64 	%rd1004, {%r948, %r947};
	xor.b64  	%rd1005, %rd1002, %rd993;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r949}, %rd1005;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r950,%dummy}, %rd1005;
	}
	shf.l.wrap.b32 	%r951, %r950, %r949, 1;
	shf.l.wrap.b32 	%r952, %r949, %r950, 1;
	mov.b64 	%rd1006, {%r952, %r951};
	add.s64 	%rd1007, %rd969, %rd1;
	add.s64 	%rd1008, %rd1007, %rd1006;
	add.s64 	%rd1009, %rd970, %rd9;
	add.s64 	%rd1010, %rd1009, %rd978;
	xor.b64  	%rd1011, %rd1008, %rd974;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1011, 32;
	shr.b64 	%rhs, %rd1011, 32;
	add.u64 	%rd1012, %lhs, %rhs;
	}
	xor.b64  	%rd1013, %rd1010, %rd998;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1013, 32;
	shr.b64 	%rhs, %rd1013, 32;
	add.u64 	%rd1014, %lhs, %rhs;
	}
	add.s64 	%rd1015, %rd1012, %rd1001;
	add.s64 	%rd1016, %rd1014, %rd1002;
	xor.b64  	%rd1017, %rd1015, %rd1006;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r953,%dummy}, %rd1017;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r954}, %rd1017;
	}
	shf.r.wrap.b32 	%r955, %r954, %r953, 24;
	shf.r.wrap.b32 	%r956, %r953, %r954, 24;
	mov.b64 	%rd1018, {%r956, %r955};
	xor.b64  	%rd1019, %rd1016, %rd978;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r957,%dummy}, %rd1019;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r958}, %rd1019;
	}
	shf.r.wrap.b32 	%r959, %r958, %r957, 24;
	shf.r.wrap.b32 	%r960, %r957, %r958, 24;
	mov.b64 	%rd1020, {%r960, %r959};
	add.s64 	%rd1021, %rd1008, %rd22;
	add.s64 	%rd1022, %rd1021, %rd1018;
	add.s64 	%rd1023, %rd1010, %rd24;
	add.s64 	%rd1024, %rd1023, %rd1020;
	xor.b64  	%rd1025, %rd1022, %rd1012;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r961,%dummy}, %rd1025;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r962}, %rd1025;
	}
	shf.r.wrap.b32 	%r963, %r962, %r961, 16;
	shf.r.wrap.b32 	%r964, %r961, %r962, 16;
	mov.b64 	%rd1026, {%r964, %r963};
	xor.b64  	%rd1027, %rd1024, %rd1014;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r965,%dummy}, %rd1027;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r966}, %rd1027;
	}
	shf.r.wrap.b32 	%r967, %r966, %r965, 16;
	shf.r.wrap.b32 	%r968, %r965, %r966, 16;
	mov.b64 	%rd1028, {%r968, %r967};
	add.s64 	%rd1029, %rd1026, %rd1015;
	add.s64 	%rd1030, %rd1028, %rd1016;
	xor.b64  	%rd1031, %rd1029, %rd1018;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r969}, %rd1031;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r970,%dummy}, %rd1031;
	}
	shf.l.wrap.b32 	%r971, %r970, %r969, 1;
	shf.l.wrap.b32 	%r972, %r969, %r970, 1;
	mov.b64 	%rd1032, {%r972, %r971};
	xor.b64  	%rd1033, %rd1030, %rd1020;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r973}, %rd1033;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r974,%dummy}, %rd1033;
	}
	shf.l.wrap.b32 	%r975, %r974, %r973, 1;
	shf.l.wrap.b32 	%r976, %r973, %r974, 1;
	mov.b64 	%rd1034, {%r976, %r975};
	add.s64 	%rd1035, %rd994, %rd37;
	add.s64 	%rd1036, %rd1035, %rd980;
	add.s64 	%rd1037, %rd1004, %rd996;
	xor.b64  	%rd1038, %rd1036, %rd1000;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1038, 32;
	shr.b64 	%rhs, %rd1038, 32;
	add.u64 	%rd1039, %lhs, %rhs;
	}
	xor.b64  	%rd1040, %rd1037, %rd972;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1040, 32;
	shr.b64 	%rhs, %rd1040, 32;
	add.u64 	%rd1041, %lhs, %rhs;
	}
	add.s64 	%rd1042, %rd1039, %rd975;
	add.s64 	%rd1043, %rd1041, %rd976;
	xor.b64  	%rd1044, %rd1042, %rd980;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r977,%dummy}, %rd1044;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r978}, %rd1044;
	}
	shf.r.wrap.b32 	%r979, %r978, %r977, 24;
	shf.r.wrap.b32 	%r980, %r977, %r978, 24;
	mov.b64 	%rd1045, {%r980, %r979};
	xor.b64  	%rd1046, %rd1043, %rd1004;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r981,%dummy}, %rd1046;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r982}, %rd1046;
	}
	shf.r.wrap.b32 	%r983, %r982, %r981, 24;
	shf.r.wrap.b32 	%r984, %r981, %r982, 24;
	mov.b64 	%rd1047, {%r984, %r983};
	add.s64 	%rd1048, %rd1045, %rd1036;
	add.s64 	%rd1049, %rd1047, %rd1037;
	xor.b64  	%rd1050, %rd1048, %rd1039;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r985,%dummy}, %rd1050;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r986}, %rd1050;
	}
	shf.r.wrap.b32 	%r987, %r986, %r985, 16;
	shf.r.wrap.b32 	%r988, %r985, %r986, 16;
	mov.b64 	%rd1051, {%r988, %r987};
	xor.b64  	%rd1052, %rd1049, %rd1041;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r989,%dummy}, %rd1052;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r990}, %rd1052;
	}
	shf.r.wrap.b32 	%r991, %r990, %r989, 16;
	shf.r.wrap.b32 	%r992, %r989, %r990, 16;
	mov.b64 	%rd1053, {%r992, %r991};
	add.s64 	%rd1054, %rd1051, %rd1042;
	add.s64 	%rd1055, %rd1053, %rd1043;
	xor.b64  	%rd1056, %rd1054, %rd1045;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r993}, %rd1056;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r994,%dummy}, %rd1056;
	}
	shf.l.wrap.b32 	%r995, %r994, %r993, 1;
	shf.l.wrap.b32 	%r996, %r993, %r994, 1;
	mov.b64 	%rd1057, {%r996, %r995};
	xor.b64  	%rd1058, %rd1055, %rd1047;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r997}, %rd1058;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r998,%dummy}, %rd1058;
	}
	shf.l.wrap.b32 	%r999, %r998, %r997, 1;
	shf.l.wrap.b32 	%r1000, %r997, %r998, 1;
	mov.b64 	%rd1059, {%r1000, %r999};
	add.s64 	%rd1060, %rd1034, %rd1022;
	add.s64 	%rd1061, %rd1057, %rd1024;
	xor.b64  	%rd1062, %rd1060, %rd1053;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1062, 32;
	shr.b64 	%rhs, %rd1062, 32;
	add.u64 	%rd1063, %lhs, %rhs;
	}
	xor.b64  	%rd1064, %rd1061, %rd1026;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1064, 32;
	shr.b64 	%rhs, %rd1064, 32;
	add.u64 	%rd1065, %lhs, %rhs;
	}
	add.s64 	%rd1066, %rd1063, %rd1054;
	add.s64 	%rd1067, %rd1065, %rd1055;
	xor.b64  	%rd1068, %rd1066, %rd1034;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1001,%dummy}, %rd1068;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1002}, %rd1068;
	}
	shf.r.wrap.b32 	%r1003, %r1002, %r1001, 24;
	shf.r.wrap.b32 	%r1004, %r1001, %r1002, 24;
	mov.b64 	%rd1069, {%r1004, %r1003};
	xor.b64  	%rd1070, %rd1067, %rd1057;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1005,%dummy}, %rd1070;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1006}, %rd1070;
	}
	shf.r.wrap.b32 	%r1007, %r1006, %r1005, 24;
	shf.r.wrap.b32 	%r1008, %r1005, %r1006, 24;
	mov.b64 	%rd1071, {%r1008, %r1007};
	add.s64 	%rd1072, %rd1069, %rd1060;
	add.s64 	%rd1073, %rd1071, %rd1061;
	xor.b64  	%rd1074, %rd1072, %rd1063;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1009,%dummy}, %rd1074;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1010}, %rd1074;
	}
	shf.r.wrap.b32 	%r1011, %r1010, %r1009, 16;
	shf.r.wrap.b32 	%r1012, %r1009, %r1010, 16;
	mov.b64 	%rd1075, {%r1012, %r1011};
	xor.b64  	%rd1076, %rd1073, %rd1065;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1013,%dummy}, %rd1076;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1014}, %rd1076;
	}
	shf.r.wrap.b32 	%r1015, %r1014, %r1013, 16;
	shf.r.wrap.b32 	%r1016, %r1013, %r1014, 16;
	mov.b64 	%rd1077, {%r1016, %r1015};
	add.s64 	%rd1078, %rd1075, %rd1066;
	add.s64 	%rd1079, %rd1077, %rd1067;
	xor.b64  	%rd1080, %rd1078, %rd1069;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1017}, %rd1080;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1018,%dummy}, %rd1080;
	}
	shf.l.wrap.b32 	%r1019, %r1018, %r1017, 1;
	shf.l.wrap.b32 	%r1020, %r1017, %r1018, 1;
	mov.b64 	%rd1081, {%r1020, %r1019};
	xor.b64  	%rd1082, %rd1079, %rd1071;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1021}, %rd1082;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1022,%dummy}, %rd1082;
	}
	shf.l.wrap.b32 	%r1023, %r1022, %r1021, 1;
	shf.l.wrap.b32 	%r1024, %r1021, %r1022, 1;
	mov.b64 	%rd1083, {%r1024, %r1023};
	add.s64 	%rd1084, %rd1059, %rd1048;
	add.s64 	%rd1085, %rd1032, %rd1049;
	xor.b64  	%rd1086, %rd1084, %rd1028;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1086, 32;
	shr.b64 	%rhs, %rd1086, 32;
	add.u64 	%rd1087, %lhs, %rhs;
	}
	xor.b64  	%rd1088, %rd1085, %rd1051;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1088, 32;
	shr.b64 	%rhs, %rd1088, 32;
	add.u64 	%rd1089, %lhs, %rhs;
	}
	add.s64 	%rd1090, %rd1087, %rd1029;
	add.s64 	%rd1091, %rd1089, %rd1030;
	xor.b64  	%rd1092, %rd1090, %rd1059;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1025,%dummy}, %rd1092;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1026}, %rd1092;
	}
	shf.r.wrap.b32 	%r1027, %r1026, %r1025, 24;
	shf.r.wrap.b32 	%r1028, %r1025, %r1026, 24;
	mov.b64 	%rd1093, {%r1028, %r1027};
	xor.b64  	%rd1094, %rd1091, %rd1032;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1029,%dummy}, %rd1094;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1030}, %rd1094;
	}
	shf.r.wrap.b32 	%r1031, %r1030, %r1029, 24;
	shf.r.wrap.b32 	%r1032, %r1029, %r1030, 24;
	mov.b64 	%rd1095, {%r1032, %r1031};
	add.s64 	%rd1096, %rd1093, %rd1084;
	add.s64 	%rd1097, %rd1095, %rd1085;
	xor.b64  	%rd1098, %rd1096, %rd1087;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1033,%dummy}, %rd1098;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1034}, %rd1098;
	}
	shf.r.wrap.b32 	%r1035, %r1034, %r1033, 16;
	shf.r.wrap.b32 	%r1036, %r1033, %r1034, 16;
	mov.b64 	%rd1099, {%r1036, %r1035};
	xor.b64  	%rd1100, %rd1097, %rd1089;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1037,%dummy}, %rd1100;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1038}, %rd1100;
	}
	shf.r.wrap.b32 	%r1039, %r1038, %r1037, 16;
	shf.r.wrap.b32 	%r1040, %r1037, %r1038, 16;
	mov.b64 	%rd1101, {%r1040, %r1039};
	add.s64 	%rd1102, %rd1099, %rd1090;
	add.s64 	%rd1103, %rd1101, %rd1091;
	xor.b64  	%rd1104, %rd1102, %rd1093;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1041}, %rd1104;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1042,%dummy}, %rd1104;
	}
	shf.l.wrap.b32 	%r1043, %r1042, %r1041, 1;
	shf.l.wrap.b32 	%r1044, %r1041, %r1042, 1;
	mov.b64 	%rd1105, {%r1044, %r1043};
	xor.b64  	%rd1106, %rd1103, %rd1095;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1045}, %rd1106;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1046,%dummy}, %rd1106;
	}
	shf.l.wrap.b32 	%r1047, %r1046, %r1045, 1;
	shf.l.wrap.b32 	%r1048, %r1045, %r1046, 1;
	mov.b64 	%rd1107, {%r1048, %r1047};
	add.s64 	%rd1108, %rd1107, %rd1072;
	add.s64 	%rd1109, %rd1073, %rd37;
	add.s64 	%rd1110, %rd1109, %rd1081;
	xor.b64  	%rd1111, %rd1108, %rd1077;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1111, 32;
	shr.b64 	%rhs, %rd1111, 32;
	add.u64 	%rd1112, %lhs, %rhs;
	}
	xor.b64  	%rd1113, %rd1110, %rd1099;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1113, 32;
	shr.b64 	%rhs, %rd1113, 32;
	add.u64 	%rd1114, %lhs, %rhs;
	}
	add.s64 	%rd1115, %rd1112, %rd1102;
	add.s64 	%rd1116, %rd1114, %rd1103;
	xor.b64  	%rd1117, %rd1115, %rd1107;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1049,%dummy}, %rd1117;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1050}, %rd1117;
	}
	shf.r.wrap.b32 	%r1051, %r1050, %r1049, 24;
	shf.r.wrap.b32 	%r1052, %r1049, %r1050, 24;
	mov.b64 	%rd1118, {%r1052, %r1051};
	xor.b64  	%rd1119, %rd1116, %rd1081;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1053,%dummy}, %rd1119;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1054}, %rd1119;
	}
	shf.r.wrap.b32 	%r1055, %r1054, %r1053, 24;
	shf.r.wrap.b32 	%r1056, %r1053, %r1054, 24;
	mov.b64 	%rd1120, {%r1056, %r1055};
	add.s64 	%rd1121, %rd1118, %rd1108;
	add.s64 	%rd1122, %rd1120, %rd1110;
	xor.b64  	%rd1123, %rd1121, %rd1112;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1057,%dummy}, %rd1123;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1058}, %rd1123;
	}
	shf.r.wrap.b32 	%r1059, %r1058, %r1057, 16;
	shf.r.wrap.b32 	%r1060, %r1057, %r1058, 16;
	mov.b64 	%rd1124, {%r1060, %r1059};
	xor.b64  	%rd1125, %rd1122, %rd1114;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1061,%dummy}, %rd1125;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1062}, %rd1125;
	}
	shf.r.wrap.b32 	%r1063, %r1062, %r1061, 16;
	shf.r.wrap.b32 	%r1064, %r1061, %r1062, 16;
	mov.b64 	%rd1126, {%r1064, %r1063};
	add.s64 	%rd1127, %rd1124, %rd1115;
	add.s64 	%rd1128, %rd1126, %rd1116;
	xor.b64  	%rd1129, %rd1128, %rd1120;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1065}, %rd1129;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1066,%dummy}, %rd1129;
	}
	shf.l.wrap.b32 	%r1067, %r1066, %r1065, 1;
	shf.l.wrap.b32 	%r1068, %r1065, %r1066, 1;
	mov.b64 	%rd1130, {%r1068, %r1067};
	add.s64 	%rd1131, %rd1083, %rd1096;
	add.s64 	%rd1132, %rd1105, %rd1097;
	xor.b64  	%rd1133, %rd1131, %rd1101;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1133, 32;
	shr.b64 	%rhs, %rd1133, 32;
	add.u64 	%rd1134, %lhs, %rhs;
	}
	xor.b64  	%rd1135, %rd1132, %rd1075;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1135, 32;
	shr.b64 	%rhs, %rd1135, 32;
	add.u64 	%rd1136, %lhs, %rhs;
	}
	add.s64 	%rd1137, %rd1134, %rd1078;
	add.s64 	%rd1138, %rd1136, %rd1079;
	xor.b64  	%rd1139, %rd1137, %rd1083;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1069,%dummy}, %rd1139;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1070}, %rd1139;
	}
	shf.r.wrap.b32 	%r1071, %r1070, %r1069, 24;
	shf.r.wrap.b32 	%r1072, %r1069, %r1070, 24;
	mov.b64 	%rd1140, {%r1072, %r1071};
	xor.b64  	%rd1141, %rd1138, %rd1105;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1073,%dummy}, %rd1141;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1074}, %rd1141;
	}
	shf.r.wrap.b32 	%r1075, %r1074, %r1073, 24;
	shf.r.wrap.b32 	%r1076, %r1073, %r1074, 24;
	mov.b64 	%rd1142, {%r1076, %r1075};
	add.s64 	%rd1143, %rd1140, %rd1131;
	add.s64 	%rd1144, %rd1142, %rd1132;
	xor.b64  	%rd1145, %rd1143, %rd1134;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1077,%dummy}, %rd1145;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1078}, %rd1145;
	}
	shf.r.wrap.b32 	%r1079, %r1078, %r1077, 16;
	shf.r.wrap.b32 	%r1080, %r1077, %r1078, 16;
	mov.b64 	%rd1146, {%r1080, %r1079};
	xor.b64  	%rd1147, %rd1144, %rd1136;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1081,%dummy}, %rd1147;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1082}, %rd1147;
	}
	shf.r.wrap.b32 	%r1083, %r1082, %r1081, 16;
	shf.r.wrap.b32 	%r1084, %r1081, %r1082, 16;
	mov.b64 	%rd1148, {%r1084, %r1083};
	add.s64 	%rd1149, %rd1146, %rd1137;
	add.s64 	%rd1150, %rd1148, %rd1138;
	xor.b64  	%rd1151, %rd1150, %rd1142;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1085}, %rd1151;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1086,%dummy}, %rd1151;
	}
	shf.l.wrap.b32 	%r1087, %r1086, %r1085, 1;
	shf.l.wrap.b32 	%r1088, %r1085, %r1086, 1;
	mov.b64 	%rd1152, {%r1088, %r1087};
	add.s64 	%rd1153, %rd1121, %rd22;
	add.s64 	%rd1154, %rd1153, %rd1130;
	xor.b64  	%rd1155, %rd1154, %rd1148;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1155, 32;
	shr.b64 	%rhs, %rd1155, 32;
	add.u64 	%rd1156, %lhs, %rhs;
	}
	add.s64 	%rd1157, %rd1149, %rd1156;
	xor.b64  	%rd1158, %rd1157, %rd1130;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1089,%dummy}, %rd1158;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1090}, %rd1158;
	}
	shf.r.wrap.b32 	%r1091, %r1090, %r1089, 24;
	shf.r.wrap.b32 	%r1092, %r1089, %r1090, 24;
	mov.b64 	%rd1159, {%r1092, %r1091};
	add.s64 	%rd1160, %rd1159, %rd1154;
	add.s64 	%rd1161, %rd1152, %rd1143;
	xor.b64  	%rd1162, %rd1126, %rd1161;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd1162, 32;
	shr.b64 	%rhs, %rd1162, 32;
	add.u64 	%rd1163, %lhs, %rhs;
	}
	add.s64 	%rd1164, %rd1127, %rd1163;
	xor.b64  	%rd1165, %rd1164, %rd1152;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1093,%dummy}, %rd1165;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1094}, %rd1165;
	}
	shf.r.wrap.b32 	%r1095, %r1094, %r1093, 24;
	shf.r.wrap.b32 	%r1096, %r1093, %r1094, 24;
	mov.b64 	%rd1166, {%r1096, %r1095};
	add.s64 	%rd1167, %rd1166, %rd1161;
	xor.b64  	%rd1168, %rd1167, %rd1163;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1097,%dummy}, %rd1168;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1098}, %rd1168;
	}
	shf.r.wrap.b32 	%r1099, %r1098, %r1097, 16;
	shf.r.wrap.b32 	%r1100, %r1097, %r1098, 16;
	mov.b64 	%rd1169, {%r1100, %r1099};
	add.s64 	%rd1170, %rd1169, %rd1164;
	xor.b64  	%rd1171, %rd1170, %rd1160;
	xor.b64  	%rd1172, %rd1171, 7640891576939301120;
	setp.lt.u64 	%p1, %rd1172, %rd5;
	@%p1 bra 	$L__BB0_2;

	ld.param.u64 	%rd1174, [nano_work_param_1];
	cvta.to.global.u64 	%rd1173, %rd1174;
	st.volatile.global.u64 	[%rd1173], %rd1;

$L__BB0_2:
	ret;

}

